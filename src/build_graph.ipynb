{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "# from pickletools import optimize\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from math import log\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from nltk.corpus import stopwords\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from torch import Tensor, nn\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\n",
    "import pickle as pkl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=148\n",
    "print(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_corpus(dataset):\n",
    "    input1 = os.sep.join(['data', dataset])\n",
    "    doc_name_list = []\n",
    "    doc_train_list = []\n",
    "    doc_test_list = []\n",
    "\n",
    "    f = open(input1 + '.txt', 'r', encoding='latin1')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "    f.close()\n",
    "\n",
    "    doc_content_list = []\n",
    "    f = open(input1 + '.clean.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "    f.close()\n",
    "\n",
    "    train_ids = []\n",
    "    for train_name in doc_train_list:\n",
    "        train_id = doc_name_list.index(train_name)\n",
    "        train_ids.append(train_id)\n",
    "    random.shuffle(train_ids)\n",
    "\n",
    "    train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "\n",
    "    test_ids = []\n",
    "    for test_name in doc_test_list:\n",
    "        test_id = doc_name_list.index(test_name)\n",
    "        test_ids.append(test_id)\n",
    "    # print(test_ids)\n",
    "    random.shuffle(test_ids)\n",
    "\n",
    "    test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "\n",
    "    ids = train_ids + test_ids\n",
    "    # print(ids)\n",
    "    # print(len(ids))\n",
    "\n",
    "    shuffle_doc_name_list = []\n",
    "    shuffle_doc_words_list = []\n",
    "    for id in ids:\n",
    "        shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "        shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "    label_set = set()\n",
    "    for doc_meta in shuffle_doc_name_list:\n",
    "        temp = doc_meta.split('\\t')\n",
    "        label_set.add(temp[2])\n",
    "    label_list = list(label_set)\n",
    "    labels = []\n",
    "    for one in shuffle_doc_name_list:\n",
    "        entry = one.split('\\t')\n",
    "        labels.append(label_list.index(entry[-1]))\n",
    "    shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "    shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "    word_freq = {}\n",
    "    word_set = set()\n",
    "    for doc_words in shuffle_doc_words_list:\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_set.add(word)\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "    vocab = list(word_set)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_doc_list = {}\n",
    "\n",
    "    for i in range(len(shuffle_doc_words_list)):\n",
    "        doc_words = shuffle_doc_words_list[i]\n",
    "        words = doc_words.split()\n",
    "        appeared = set()\n",
    "        for word in words:\n",
    "            if word in appeared:\n",
    "                continue\n",
    "            if word in word_doc_list:\n",
    "                doc_list = word_doc_list[word]\n",
    "                doc_list.append(i)\n",
    "                word_doc_list[word] = doc_list\n",
    "            else:\n",
    "                word_doc_list[word] = [i]\n",
    "            appeared.add(word)\n",
    "\n",
    "    word_doc_freq = {}\n",
    "    for word, doc_list in word_doc_list.items():\n",
    "        word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "    word_id_map = {}\n",
    "    id_word_map = {}\n",
    "    for i in range(vocab_size):\n",
    "        word_id_map[vocab[i]] = i\n",
    "        id_word_map[i] = vocab[i]\n",
    "\n",
    "    return shuffle_doc_name_list, shuffle_doc_words_list, train_ids, test_ids, word_doc_freq, word_id_map, id_word_map, vocab, labels, label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tfidf(corpus, word_id_map, word_doc_freq, vocab, train_size):\n",
    "    row, col, weight_tfidf = [],[],[]\n",
    "    vocab_size = len(vocab)\n",
    "    doc_word_freq = {}\n",
    "    for doc_id in range(len(corpus)):\n",
    "        doc_words = corpus[doc_id]\n",
    "        words = doc_words.split()\n",
    "        for word in words:\n",
    "            word_id = word_id_map[word]\n",
    "            doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "            if doc_word_str in doc_word_freq:\n",
    "                doc_word_freq[doc_word_str] += 1\n",
    "            else:\n",
    "                doc_word_freq[doc_word_str] = 1\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        doc_words = corpus[i]\n",
    "        words = doc_words.split()\n",
    "        doc_word_set = set()\n",
    "        for word in words:\n",
    "            if word in doc_word_set:\n",
    "                continue\n",
    "            j = word_id_map[word]\n",
    "            key = str(i) + ',' + str(j)\n",
    "            freq = doc_word_freq[key]\n",
    "            if i < train_size:\n",
    "                row.append(i)\n",
    "            else:\n",
    "                row.append(i + vocab_size)\n",
    "            col.append(train_size + j)\n",
    "            idf = log(1.0 * len(corpus) /\n",
    "                    word_doc_freq[vocab[j]])\n",
    "            weight_tfidf.append(freq * idf)\n",
    "            doc_word_set.add(word)\n",
    "    return row, col, weight_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "name, corpus, train_ids, test_ids, word_doc_freq, word_id_map, id_word_map, vocab, labels, label_list = gen_corpus(\"20ng\")\n",
    "data = [train_ids, test_ids, corpus, labels, vocab, word_id_map, id_word_map, label_list]\n",
    "\n",
    "num_labels = len(label_list)\n",
    "row_tfidf, col_tfidf, weight_tfidf = gen_tfidf(corpus, word_id_map, word_doc_freq, vocab, len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(0.1*len(train_ids))\n",
    "train_size = len(train_ids) - valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres=0.05\n",
    "max_len=512\n",
    "window_size=7\n",
    "lr=1e-3\n",
    "batch_size=32\n",
    "embed_size=200\n",
    "hidden_size=200\n",
    "dropout=0\n",
    "weight_decay=1e-6\n",
    "epochs=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_corpus_to_ids(corpus, word_id_map, max_len):\n",
    "    new_corpus = []\n",
    "    for text in corpus:\n",
    "        word_list = text.split()\n",
    "        if len(word_list) > max_len:\n",
    "            word_list = word_list[:max_len]\n",
    "        new_corpus.append([word_id_map[w] + 1 for w in word_list]) # + 1 for padding\n",
    "    # padding\n",
    "    for i, one in enumerate(new_corpus):\n",
    "        if len(one) < max_len:\n",
    "            new_corpus[i] = one + [0]*(max_len-len(one))\n",
    "    new_corpus = np.asarray(new_corpus, dtype=np.int32)\n",
    "    return new_corpus\n",
    "\n",
    "def lstm_eval(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels,all_outs = [],[],[]\n",
    "    for batch in dataloader:\n",
    "        batch = [one.to(device) for one in batch]\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            output, pred = model(x)\n",
    "            all_outs.append(output.cpu().numpy())\n",
    "            pred_ids = torch.argmax(pred, dim=-1)\n",
    "            all_preds += pred_ids.tolist()\n",
    "            all_labels += y.tolist()\n",
    "    acc = np.mean(np.asarray(all_preds) == np.asarray(all_labels))\n",
    "    all_outs = np.concatenate(all_outs, axis=0)\n",
    "\n",
    "    model.train()\n",
    "    return acc, all_outs\n",
    "\n",
    "class LSTM_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_labels, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size,num_layers=1, batch_first=True, dropout=dropout, bidirectional=False)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "    def forward(self, inputs):\n",
    "        emb = self.embedding(inputs)\n",
    "        output, (h_n, c_n) = self.lstm(emb)\n",
    "        inter_output = torch.mean(output, dim=1)\n",
    "        res = self.classifier(inter_output)\n",
    "        return output, res\n",
    "\n",
    "def train_lstm(corpus, word_id_map, train_size, valid_size, labels, emb_size, hidden_size, dropout, batch_size, epochs, lr, weight_decay, num_labels,device,max_len):\n",
    "    vocab_size = len(word_id_map) + 1\n",
    "    corpus_ids = trans_corpus_to_ids(corpus, word_id_map, max_len)\n",
    "    model = LSTM_classifier(vocab_size, emb_size, hidden_size, num_labels, dropout)\n",
    "    model.to(device)\n",
    "    train_data = corpus_ids[:train_size,:]\n",
    "    dev_data = corpus_ids[train_size:train_size+valid_size,:]\n",
    "    test_data = corpus_ids[train_size+valid_size:,:]\n",
    "    train_label = labels[:train_size]\n",
    "    dev_label = labels[train_size:train_size+valid_size]\n",
    "    test_label = labels[train_size+valid_size:]\n",
    "    train_x = torch.tensor(train_data, dtype=torch.long)\n",
    "    train_y = torch.tensor(train_label, dtype=torch.long)\n",
    "    dev_x = torch.tensor(dev_data, dtype=torch.long)\n",
    "    dev_y = torch.tensor(dev_label, dtype=torch.long)\n",
    "    test_x = torch.tensor(test_data, dtype=torch.long)\n",
    "    test_y = torch.tensor(test_label, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    dev_dataset = TensorDataset(dev_x, dev_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dev_sampler = SequentialSampler(train_dataset)\n",
    "    dev_sampler = SequentialSampler(dev_dataset)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size,sampler=train_sampler)\n",
    "    train_dev_dataloader = DataLoader(train_dataset,batch_size,sampler=train_dev_sampler)\n",
    "    dev_dataloader = DataLoader(dev_dataset,batch_size,sampler=dev_sampler)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size,sampler=test_sampler)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "    best_acc = 0.0\n",
    "    if epochs > 0:\n",
    "        for ep in range(epochs):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                batch = [one.to(device) for one in batch]\n",
    "                x, y = batch\n",
    "                output, pred = model(x)\n",
    "                loss = loss_func(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            acc, all_outs = lstm_eval(model, dev_dataloader, device)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                print(\"Saving semantic model into lstm1.bin\")\n",
    "                torch.save(model.state_dict(), 'lstm1.bin')\n",
    "                print(\"current best acc={:4f}\".format(acc))\n",
    "    else:\n",
    "        print(\"loading lstm model\")\n",
    "        model.load_state_dict(torch.load('lstm.bin'))\n",
    "    acc, all_outs_train = lstm_eval(model, train_dev_dataloader, device)\n",
    "    acc, all_outs_dev = lstm_eval(model, dev_dataloader, device)\n",
    "    acc, all_outs_test = lstm_eval(model, test_dataloader, device)\n",
    "    all_outs = np.concatenate([all_outs_train, all_outs_dev, all_outs_test], axis=0)\n",
    "    return model, all_outs, corpus_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(corpus, word_id_map, train_size, valid_size, labels, emb_size, hidden_size, dropout, batch_size, epochs, lr, weight_decay, num_labels,device,max_len):\n",
    "    vocab_size = len(word_id_map) + 1\n",
    "    corpus_ids = trans_corpus_to_ids(corpus, word_id_map, max_len)\n",
    "    model = LSTM_classifier(vocab_size, emb_size, hidden_size, num_labels, dropout)\n",
    "    model.to(device)\n",
    "    train_data = corpus_ids[:train_size,:]\n",
    "    dev_data = corpus_ids[train_size:train_size+valid_size,:]\n",
    "    test_data = corpus_ids[train_size+valid_size:,:]\n",
    "    train_label = labels[:train_size]\n",
    "    dev_label = labels[train_size:train_size+valid_size]\n",
    "    test_label = labels[train_size+valid_size:]\n",
    "    train_x = torch.tensor(train_data, dtype=torch.long)\n",
    "    train_y = torch.tensor(train_label, dtype=torch.long)\n",
    "    dev_x = torch.tensor(dev_data, dtype=torch.long)\n",
    "    dev_y = torch.tensor(dev_label, dtype=torch.long)\n",
    "    test_x = torch.tensor(test_data, dtype=torch.long)\n",
    "    test_y = torch.tensor(test_label, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    dev_dataset = TensorDataset(dev_x, dev_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dev_sampler = SequentialSampler(train_dataset)\n",
    "    dev_sampler = SequentialSampler(dev_dataset)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size,sampler=train_sampler)\n",
    "    train_dev_dataloader = DataLoader(train_dataset,batch_size,sampler=train_dev_sampler)\n",
    "    dev_dataloader = DataLoader(dev_dataset,batch_size,sampler=dev_sampler)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size,sampler=test_sampler)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.train()\n",
    "    loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "    best_acc = 0.0\n",
    "    if epochs > 0:\n",
    "        for ep in range(epochs):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                batch = [one.to(device) for one in batch]\n",
    "                x, y = batch\n",
    "                output, pred = model(x)\n",
    "                loss = loss_func(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            acc, all_outs = lstm_eval(model, dev_dataloader, device)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                print(\"Saving semantic model into lstm1.bin\")\n",
    "                torch.save(model.state_dict(), 'lstm1.bin')\n",
    "                print(\"current best acc={:4f}\".format(acc))\n",
    "    else:\n",
    "        print(\"loading lstm model\")\n",
    "        model.load_state_dict(torch.load('lstm.bin'))\n",
    "    acc, all_outs_train = lstm_eval(model, train_dev_dataloader, device)\n",
    "    acc, all_outs_dev = lstm_eval(model, dev_dataloader, device)\n",
    "    acc, all_outs_test = lstm_eval(model, test_dataloader, device)\n",
    "    all_outs = np.concatenate([all_outs_train, all_outs_dev, all_outs_test], axis=0)\n",
    "    return model, all_outs, corpus_ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_id_map) + 1\n",
    "corpus_ids = trans_corpus_to_ids(corpus, word_id_map, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lstm model\n"
     ]
    }
   ],
   "source": [
    "model, all_outs, corpus_ids = train_lstm(corpus, word_id_map, train_size, valid_size, labels, embed_size, hidden_size, dropout, batch_size, epochs, lr, weight_decay, num_labels,device, max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
