{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Li Zefeng \n",
        " G2204688A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSLMMr1kpCGL",
        "outputId": "41a9bf7f-965e-4176-d109-d1535c13e807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7a2IZ8rpxcA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpB7rMwpeVTu"
      },
      "outputs": [],
      "source": [
        "!pip install stanfordcorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkgNWUXRm6K4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59zgYCrNd6xt"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "# from pickletools import optimize\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import json\n",
        "from math import log\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # del\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from nltk.corpus import stopwords\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from torch import Tensor, nn\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class LSTM_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size, num_labels, dropout, num_layers=1) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size,num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "    def forward(self, inputs):\n",
        "        emb = self.embedding(inputs)\n",
        "        output, (h_n, c_n) = self.lstm(emb)\n",
        "        inter_output = torch.mean(output, dim=1)\n",
        "        res = self.classifier(inter_output)\n",
        "        return output, res\n",
        "\n",
        "\n",
        "\n",
        "def gen_syn(corpus, nlp:StanfordCoreNLP, row_tfidf, col_tfidf, weight_tfidf, word_id_map, node_size, train_size):\n",
        "    '''\n",
        "    calculate syntactic relationship over words in the corpus\n",
        "    input:\n",
        "        corpus: a list that contains sentences/documents (strings)\n",
        "        pmi: a dict that maps word pair to pmi\n",
        "    '''\n",
        "    t = time.time()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    #获取句法依存关系对\n",
        "    rela_pair_count_str = {}\n",
        "    for doc_id in tqdm(range(len(corpus))):\n",
        "        # print(doc_id)\n",
        "        words = corpus[doc_id]\n",
        "        words = words.split(\"\\n\")\n",
        "        rela=[]\n",
        "        for window in words:\n",
        "            if not window.strip():\n",
        "                continue\n",
        "            #构造rela_pair_count\n",
        "            window = window.replace(string.punctuation, ' ')\n",
        "            try:\n",
        "                r_dict = nlp._request('depparse', window)\n",
        "            except json.decoder.JSONDecodeError:\n",
        "                continue\n",
        "            res = [(dep['governorGloss'], dep['dependentGloss']) for s in r_dict['sentences'] for dep in\n",
        "            s['basicDependencies']]\n",
        "            for tuple in res:\n",
        "                rela.append(tuple[0] + ', ' + str(tuple[1]))\n",
        "            for pair in rela:\n",
        "                pair=pair.split(\", \")\n",
        "                if pair[0]=='ROOT' or pair[1]=='ROOT':\n",
        "                    continue\n",
        "                if pair[0] == pair[1]:\n",
        "                    continue\n",
        "                if pair[0] in string.punctuation or pair[1] in string.punctuation:\n",
        "                    continue\n",
        "                if pair[0] in stop_words or pair[1] in stop_words:\n",
        "                    continue\n",
        "                word_pair_str = pair[0] + ',' + pair[1]\n",
        "                if word_pair_str in rela_pair_count_str:\n",
        "                    rela_pair_count_str[word_pair_str] += 1\n",
        "                else:\n",
        "                    rela_pair_count_str[word_pair_str] = 1\n",
        "                # two orders\n",
        "                word_pair_str = pair[1] + ',' + pair[0]\n",
        "                if word_pair_str in rela_pair_count_str:\n",
        "                    rela_pair_count_str[word_pair_str] += 1\n",
        "                else:\n",
        "                    rela_pair_count_str[word_pair_str] = 1\n",
        "    max_count = 0\n",
        "    min_count = 1000000\n",
        "    for v in rela_pair_count_str.values():\n",
        "        if v < min_count:\n",
        "            min_count = v\n",
        "        if v > max_count:\n",
        "            max_count = v\n",
        "    graph = []\n",
        "    row, col = [],[]\n",
        "    for key in rela_pair_count_str:\n",
        "        temp = key.split(',')\n",
        "        if temp[0] not in word_id_map or temp[1] not in word_id_map:\n",
        "            continue\n",
        "        i = word_id_map[temp[0]]\n",
        "        j = word_id_map[temp[1]]\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        w = (rela_pair_count_str[key] - min_count) / (max_count - min_count)\n",
        "        graph.append(w)\n",
        "    weight = graph + weight_tfidf\n",
        "    num_edges = len(row)\n",
        "    row = row + row_tfidf\n",
        "    col = col + col_tfidf\n",
        "    adj = sp.csr_matrix(\n",
        "        (weight, (row, col)), shape=(node_size, node_size))\n",
        "    logger.info(\"Syntactic graph finish! Time spent {:2f} number of edges {}\".format(time.time()-t, num_edges))\n",
        "    return adj\n",
        "\n",
        "def trans_corpus_to_ids(corpus, word_id_map, max_len):\n",
        "    new_corpus = []\n",
        "    for text in corpus:\n",
        "        word_list = text.split()\n",
        "        if len(word_list) > max_len:\n",
        "            word_list = word_list[:max_len]\n",
        "        new_corpus.append([word_id_map[w] + 1 for w in word_list]) # + 1 for padding\n",
        "    # padding\n",
        "    for i, one in enumerate(new_corpus):\n",
        "        if len(one) < max_len:\n",
        "            new_corpus[i] = one + [0]*(max_len-len(one))\n",
        "    new_corpus = np.asarray(new_corpus, dtype=np.int32)\n",
        "    return new_corpus\n",
        "\n",
        "def lstm_eval(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels,all_outs = [],[],[]\n",
        "    for batch in dataloader:\n",
        "        batch = [one.to(device) for one in batch]\n",
        "        x, y = batch\n",
        "        with torch.no_grad():\n",
        "            output, pred = model(x)\n",
        "            all_outs.append(output.cpu().numpy())\n",
        "            pred_ids = torch.argmax(pred, dim=-1)\n",
        "            all_preds += pred_ids.tolist()\n",
        "            all_labels += y.tolist()\n",
        "    acc = np.mean(np.asarray(all_preds) == np.asarray(all_labels))\n",
        "    all_outs = np.concatenate(all_outs, axis=0)\n",
        "\n",
        "    model.train()\n",
        "    return acc, all_outs\n",
        "\n",
        "def train_lstm(corpus, word_id_map, train_size, valid_size, labels, emb_size, hidden_size, dropout, batch_size, epochs, lr, weight_decay, num_labels,device,max_len, dataset, graphs_saved_path, num_layers):\n",
        "    vocab_size = len(word_id_map) + 1\n",
        "    corpus_ids = trans_corpus_to_ids(corpus, word_id_map, max_len)\n",
        "    model = LSTM_classifier(vocab_size, emb_size, hidden_size, num_labels, dropout, num_layers=num_layers)\n",
        "    model.to(device)\n",
        "    train_data = corpus_ids[:train_size,:]\n",
        "    dev_data = corpus_ids[train_size:train_size+valid_size,:]\n",
        "    test_data = corpus_ids[train_size+valid_size:,:]\n",
        "    train_label = labels[:train_size]\n",
        "    dev_label = labels[train_size:train_size+valid_size]\n",
        "    test_label = labels[train_size+valid_size:]\n",
        "    train_x = torch.tensor(train_data, dtype=torch.long)\n",
        "    train_y = torch.tensor(train_label, dtype=torch.long)\n",
        "    dev_x = torch.tensor(dev_data, dtype=torch.long)\n",
        "    dev_y = torch.tensor(dev_label, dtype=torch.long)\n",
        "    test_x = torch.tensor(test_data, dtype=torch.long)\n",
        "    test_y = torch.tensor(test_label, dtype=torch.long)\n",
        "    train_dataset = TensorDataset(train_x, train_y)\n",
        "    dev_dataset = TensorDataset(dev_x, dev_y)\n",
        "    test_dataset = TensorDataset(test_x, test_y)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dev_sampler = SequentialSampler(train_dataset)\n",
        "    dev_sampler = SequentialSampler(dev_dataset)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset,batch_size,sampler=train_sampler)\n",
        "    train_dev_dataloader = DataLoader(train_dataset,batch_size,sampler=train_dev_sampler)\n",
        "    dev_dataloader = DataLoader(dev_dataset,batch_size,sampler=dev_sampler)\n",
        "    test_dataloader = DataLoader(test_dataset,batch_size,sampler=test_sampler)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    model.train()\n",
        "    loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # training LSTM\n",
        "    if epochs > 0:\n",
        "        for ep in range(epochs):\n",
        "            logger.info(\"Starting epochs [{}/{}]\".format(ep+1, epochs))\n",
        "            for batch in tqdm(train_dataloader):\n",
        "                batch = [one.to(device) for one in batch]\n",
        "                x, y = batch\n",
        "                output, pred = model(x)\n",
        "                loss = loss_func(pred, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            acc, all_outs = lstm_eval(model, dev_dataloader, device)\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                logger.info(\"Saving semantic model into {}_lstm.bin\".format(dataset))\n",
        "\n",
        "                # create directory for saving LSTM\n",
        "                # graphs_saved_path = \"saved_graphs/run_{}\".format(timestamp)\n",
        "                # if not os.path.exists(graphs_saved_path):\n",
        "                #     os.makedirs(graphs_saved_path)\n",
        "                torch.save(model.state_dict(), os.path.join(graphs_saved_path, '{}_lstm.bin'.format(dataset)))\n",
        "                logger.info(\"current best acc={:4f}\".format(acc))\n",
        "    else:\n",
        "        # CRITICAL - this part is illogical\n",
        "        logger.info(\"loading lstm model\")\n",
        "        model.load_state_dict(torch.load('lstm.bin'))\n",
        "    acc, all_outs_train = lstm_eval(model, train_dev_dataloader, device)\n",
        "    acc, all_outs_dev = lstm_eval(model, dev_dataloader, device)\n",
        "    acc, all_outs_test = lstm_eval(model, test_dataloader, device)\n",
        "    all_outs = np.concatenate([all_outs_train, all_outs_dev, all_outs_test], axis=0)\n",
        "    return model, all_outs, corpus_ids\n",
        "\n",
        "def gen_sem(args, corpus, word_id_map, row_tfidf, col_tfidf, weight_tfidf, thres, train_size, valid_size, labels, num_labels, node_size, device, graphs_saved_path):\n",
        "\n",
        "    t = time.time()\n",
        "\n",
        "    # training LSTM\n",
        "    model, all_outs, corpus_ids = train_lstm(corpus, word_id_map, train_size, valid_size, labels, args.embed_size, args.hidden_size, args.dropout, args.batch_size, args.epochs, args.lr, args.weight_decay, num_labels,device, args.max_len, args.dataset, graphs_saved_path, args.num_layers)\n",
        "    logger.info(\"Training LSTM completed\")\n",
        "\n",
        "    logger.info(\"all_outs:\\n {}\".format(all_outs))\n",
        "\n",
        "    num_docs = all_outs.shape[0]\n",
        "    test_ids = corpus_ids[train_size+valid_size:,:]\n",
        "    cos_simi_count = {}\n",
        "    for i in tqdm(range(num_docs)):\n",
        "        text = corpus[i]\n",
        "        word_list = text.split()\n",
        "        max_len = len(word_list) if len(word_list) < args.max_len else args.max_len\n",
        "        x = all_outs[i,:,:]\n",
        "        x_norm = np.linalg.norm(x, ord=2, axis=-1, keepdims=True)\n",
        "        simi_mat = np.dot(x, x.T) / np.dot(x_norm, x_norm.T) # L * L\n",
        "        for k in range(max_len):\n",
        "            for j in range(k+1, max_len):\n",
        "                word_k_id = word_id_map[word_list[k]]\n",
        "                word_j_id = word_id_map[word_list[j]]\n",
        "                simi = simi_mat[k,j]\n",
        "                if word_k_id == word_j_id:\n",
        "                    continue\n",
        "                if simi > thres:\n",
        "                    word_pair_str = str(word_k_id) + ',' + str(word_j_id)\n",
        "                    if word_pair_str in cos_simi_count:\n",
        "                        cos_simi_count[word_pair_str] += 1\n",
        "                    else:\n",
        "                        cos_simi_count[word_pair_str] = 1\n",
        "                    # two orders\n",
        "                    word_pair_str = str(word_j_id) + ',' + str(word_k_id)\n",
        "                    if word_pair_str in cos_simi_count:\n",
        "                        cos_simi_count[word_pair_str] += 1\n",
        "                    else:\n",
        "                        cos_simi_count[word_pair_str] = 1\n",
        "\n",
        "    max_count = 0\n",
        "    min_count = 1000000\n",
        "    row, col = [],[]\n",
        "    for v in cos_simi_count.values():\n",
        "        if v < min_count:\n",
        "            min_count = v\n",
        "        if v > max_count:\n",
        "            max_count = v\n",
        "\n",
        "    graph = []\n",
        "    for key in cos_simi_count:\n",
        "        temp = key.split(',')\n",
        "        # if temp[0] not in word_id_map or temp[1] not in word_id_map:\n",
        "        #     continue\n",
        "        i = int(temp[0])\n",
        "        j = int(temp[1])\n",
        "        w = (cos_simi_count[key] - min_count) / (max_count - min_count)\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        graph.append(w)\n",
        "\n",
        "    weight = graph + weight_tfidf\n",
        "    num_edges = len(row)\n",
        "    row = row + row_tfidf\n",
        "    col = col + col_tfidf\n",
        "    adj = sp.csr_matrix(\n",
        "        (weight, (row, col)), shape=(node_size, node_size))\n",
        "    logger.info(\"Semantic graph finish! Time spent {:2f} number of edges {}\".format(time.time()-t, num_edges))\n",
        "    return adj\n",
        "\n",
        "def gen_seq(corpus, train_size, test_size, window_size, word_id_map, row_tfidf, col_tfidf, weight_tfidf, vocab):\n",
        "    windows = []\n",
        "    row, col, weight = [],[],[]\n",
        "    t = time.time()\n",
        "    vocab_size = len(vocab)\n",
        "    logger.info(\"Generating sequential graph...\")\n",
        "    logger.info(\"windows generating...\")\n",
        "    for doc_words in corpus:\n",
        "        words = doc_words.split()\n",
        "        length = len(words)\n",
        "        if length <= window_size:\n",
        "            windows.append(words)\n",
        "        else:\n",
        "            # print(length, length - window_size + 1)\n",
        "            for j in range(length - window_size + 1):\n",
        "                window = words[j: j + window_size]\n",
        "                windows.append(window)\n",
        "                # print(window)\n",
        "    logger.info(\"calculating word frequency...\")\n",
        "    word_window_freq = {}\n",
        "    for window in tqdm(windows):\n",
        "        appeared = set()\n",
        "        for i in range(len(window)):\n",
        "            if window[i] in appeared:\n",
        "                continue\n",
        "            if window[i] in word_window_freq:\n",
        "                word_window_freq[window[i]] += 1\n",
        "            else:\n",
        "                word_window_freq[window[i]] = 1\n",
        "            appeared.add(window[i])\n",
        "    logger.info(\"calculating word pair frequency...\")\n",
        "    word_pair_count = {}\n",
        "    for window in windows:\n",
        "        for i in range(1, len(window)):\n",
        "            for j in range(0, i):\n",
        "                word_i = window[i]\n",
        "                word_i_id = word_id_map[word_i]\n",
        "                word_j = window[j]\n",
        "                word_j_id = word_id_map[word_j]\n",
        "                if word_i_id == word_j_id:\n",
        "                    continue\n",
        "                word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
        "                if word_pair_str in word_pair_count:\n",
        "                    word_pair_count[word_pair_str] += 1\n",
        "                else:\n",
        "                    word_pair_count[word_pair_str] = 1\n",
        "                # two orders\n",
        "                word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
        "                if word_pair_str in word_pair_count:\n",
        "                    word_pair_count[word_pair_str] += 1\n",
        "                else:\n",
        "                    word_pair_count[word_pair_str] = 1\n",
        "    num_window = len(windows)\n",
        "    pmi_dict = {}\n",
        "    logger.info(\"calculating pmi...\")\n",
        "    for key in word_pair_count:\n",
        "        temp = key.split(',')\n",
        "        i = int(temp[0])\n",
        "        j = int(temp[1])\n",
        "        count = word_pair_count[key]\n",
        "        word_freq_i = word_window_freq[vocab[i]]\n",
        "        word_freq_j = word_window_freq[vocab[j]]\n",
        "        pmi = log((1.0 * count / num_window) /\n",
        "                (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
        "        if pmi <= 0:\n",
        "            continue\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        weight.append(pmi)\n",
        "        pmi_dict[key] = pmi\n",
        "    logger.info(\"create pmi graph...\")\n",
        "    weight = weight + weight_tfidf\n",
        "    num_edges = len(row)\n",
        "    row = row + row_tfidf\n",
        "    col = col + col_tfidf\n",
        "    node_size = train_size + vocab_size + test_size\n",
        "    adj = sp.csr_matrix(\n",
        "        (weight, (row, col)), shape=(node_size, node_size))\n",
        "    logger.info(\"Sequential graph finish! Time spent {:2f} number of edges {}\".format(time.time()-t, num_edges))\n",
        "    return pmi_dict, adj, row, col\n",
        "\n",
        "def gen_tfidf(corpus, word_id_map, word_doc_freq, vocab, train_size):\n",
        "    row, col, weight_tfidf = [],[],[]\n",
        "    vocab_size = len(vocab)\n",
        "    doc_word_freq = {}\n",
        "    for doc_id in range(len(corpus)):\n",
        "        doc_words = corpus[doc_id]\n",
        "        words = doc_words.split()\n",
        "        for word in words:\n",
        "            word_id = word_id_map[word]\n",
        "            doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "            if doc_word_str in doc_word_freq:\n",
        "                doc_word_freq[doc_word_str] += 1\n",
        "            else:\n",
        "                doc_word_freq[doc_word_str] = 1\n",
        "\n",
        "    for i in range(len(corpus)):\n",
        "        doc_words = corpus[i]\n",
        "        words = doc_words.split()\n",
        "        doc_word_set = set()\n",
        "        for word in words:\n",
        "            if word in doc_word_set:\n",
        "                continue\n",
        "            j = word_id_map[word]\n",
        "            key = str(i) + ',' + str(j)\n",
        "            freq = doc_word_freq[key]\n",
        "            if i < train_size:\n",
        "                row.append(i)\n",
        "            else:\n",
        "                row.append(i + vocab_size)\n",
        "            col.append(train_size + j)\n",
        "            idf = log(1.0 * len(corpus) /\n",
        "                    word_doc_freq[vocab[j]])\n",
        "            weight_tfidf.append(freq * idf)\n",
        "            doc_word_set.add(word)\n",
        "    return row, col, weight_tfidf\n",
        "\n",
        "def gen_corpus(dataset):\n",
        "    input1 = os.sep.join(['data', dataset])\n",
        "    doc_name_list = []\n",
        "    doc_train_list = []\n",
        "    doc_test_list = []\n",
        "\n",
        "    f = open(input1 + '.txt', 'r', encoding='latin1')\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_name_list.append(line.strip())\n",
        "        temp = line.split(\"\\t\")\n",
        "        if temp[1].find('test') != -1:\n",
        "            doc_test_list.append(line.strip())\n",
        "        elif temp[1].find('train') != -1:\n",
        "            doc_train_list.append(line.strip())\n",
        "    f.close()\n",
        "\n",
        "    doc_content_list = []\n",
        "    f = open(input1 + '.clean.txt', 'r')\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        doc_content_list.append(line.strip())\n",
        "    f.close()\n",
        "\n",
        "    train_ids = []\n",
        "    for train_name in doc_train_list:\n",
        "        train_id = doc_name_list.index(train_name)\n",
        "        train_ids.append(train_id)\n",
        "    random.shuffle(train_ids)\n",
        "\n",
        "    train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
        "\n",
        "    test_ids = []\n",
        "    for test_name in doc_test_list:\n",
        "        test_id = doc_name_list.index(test_name)\n",
        "        test_ids.append(test_id)\n",
        "    # print(test_ids)\n",
        "    random.shuffle(test_ids)\n",
        "\n",
        "    test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
        "\n",
        "    ids = train_ids + test_ids\n",
        "    # print(ids)\n",
        "    # print(len(ids))\n",
        "\n",
        "    shuffle_doc_name_list = []\n",
        "    shuffle_doc_words_list = []\n",
        "    for id in ids:\n",
        "        shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
        "        shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
        "    label_set = set()\n",
        "    for doc_meta in shuffle_doc_name_list:\n",
        "        temp = doc_meta.split('\\t')\n",
        "        label_set.add(temp[2])\n",
        "    label_list = list(label_set)\n",
        "    labels = []\n",
        "    for one in shuffle_doc_name_list:\n",
        "        entry = one.split('\\t')\n",
        "        labels.append(label_list.index(entry[-1]))\n",
        "    shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
        "    shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
        "    word_freq = {}\n",
        "    word_set = set()\n",
        "    for doc_words in shuffle_doc_words_list:\n",
        "        words = doc_words.split()\n",
        "        for word in words:\n",
        "            word_set.add(word)\n",
        "            if word in word_freq:\n",
        "                word_freq[word] += 1\n",
        "            else:\n",
        "                word_freq[word] = 1\n",
        "\n",
        "    vocab = list(word_set)\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    word_doc_list = {}\n",
        "\n",
        "    for i in range(len(shuffle_doc_words_list)):\n",
        "        doc_words = shuffle_doc_words_list[i]\n",
        "        words = doc_words.split()\n",
        "        appeared = set()\n",
        "        for word in words:\n",
        "            if word in appeared:\n",
        "                continue\n",
        "            if word in word_doc_list:\n",
        "                doc_list = word_doc_list[word]\n",
        "                doc_list.append(i)\n",
        "                word_doc_list[word] = doc_list\n",
        "            else:\n",
        "                word_doc_list[word] = [i]\n",
        "            appeared.add(word)\n",
        "\n",
        "    word_doc_freq = {}\n",
        "    for word, doc_list in word_doc_list.items():\n",
        "        word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "    word_id_map = {}\n",
        "    id_word_map = {}\n",
        "    for i in range(vocab_size):\n",
        "        word_id_map[vocab[i]] = i\n",
        "        id_word_map[i] = vocab[i]\n",
        "\n",
        "    return shuffle_doc_name_list, shuffle_doc_words_list, train_ids, test_ids, word_doc_freq, word_id_map, id_word_map, vocab, labels, label_list\n",
        "\n",
        "def main(args, timestamp):\n",
        "    # load stanfordcorenlp\n",
        "    nlp = StanfordCoreNLP(args.corenlp, lang='en')\n",
        "\n",
        "    # generate seed for reproducability\n",
        "    set_torch_seed(seed=148)\n",
        "\n",
        "    # set gpu or cpu\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        logger.info(\"Training is running on {}\".format(device))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        logger.info(\"Training is running on CPU\")\n",
        "\n",
        "    # load corpus\n",
        "    name, corpus, train_ids, test_ids, word_doc_freq, word_id_map, id_word_map, vocab, labels, label_list = gen_corpus(args.dataset)\n",
        "    data = [train_ids, test_ids, corpus, labels, vocab, word_id_map, id_word_map, label_list]\n",
        "\n",
        "    json.dump(data, open('./data/{}_data.json'.format(args.dataset),'w'))\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    row_tfidf, col_tfidf, weight_tfidf = gen_tfidf(corpus, word_id_map, word_doc_freq, vocab, len(train_ids))\n",
        "\n",
        "    # create directory for saving LSTM\n",
        "    graphs_saved_path = \"saved_graphs/run_{}\".format(timestamp)\n",
        "    argparse_dict = vars(args)\n",
        "    if not os.path.exists(graphs_saved_path):\n",
        "        os.makedirs(graphs_saved_path)\n",
        "        with open(os.path.join(graphs_saved_path, 'graph_config.json'.format(timestamp)), 'w') as fjson:\n",
        "            json.dump(argparse_dict, fjson)\n",
        "\n",
        "    # generate sequential graph if true\n",
        "    if args.gen_seq:\n",
        "        pmi_dict, seq_adj, row, col = gen_seq(corpus, len(train_ids), len(test_ids), args.window_size, word_id_map, row_tfidf, col_tfidf, weight_tfidf, vocab)\n",
        "\n",
        "        # pickle graph object\n",
        "        pickle_graph(\n",
        "            graph_type='sequential',\n",
        "            dataset=args.dataset,\n",
        "            graph_adj=seq_adj,\n",
        "            graph_saved_path=graphs_saved_path)\n",
        "\n",
        "    # generate syntatic graph if true\n",
        "    if args.gen_syn:\n",
        "        syn_adj = gen_syn(corpus, nlp, row_tfidf, col_tfidf, weight_tfidf, word_id_map, len(train_ids)+len(vocab)+len(test_ids), len(train_ids))\n",
        "\n",
        "        # pickle graph object\n",
        "        pickle_graph(\n",
        "            graph_type='syntactic',\n",
        "            dataset=args.dataset,\n",
        "            graph_adj=syn_adj,\n",
        "            graph_saved_path=graphs_saved_path)\n",
        "\n",
        "    # generate semantic graph if true\n",
        "    if args.gen_sem:\n",
        "        valid_size = int(0.1*len(train_ids))\n",
        "        train_size = len(train_ids) - valid_size\n",
        "        sem_adj = gen_sem(args, corpus, word_id_map, row_tfidf, col_tfidf, weight_tfidf, args.thres, train_size, valid_size, labels, num_labels, len(train_ids)+len(vocab)+len(test_ids),device, graphs_saved_path)\n",
        "\n",
        "        # pickle graph object\n",
        "        pickle_graph(\n",
        "            graph_type='semantic',\n",
        "            dataset=args.dataset,\n",
        "            graph_adj=sem_adj,\n",
        "            graph_saved_path=graphs_saved_path)\n",
        "\n",
        "def parse_args(args=None):\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Training and Testing Knowledge Graph Embedding Models',\n",
        "        usage='train.py [<args>] [-h | --help]'\n",
        "    )\n",
        "    parser.add_argument('--gen_syn', default=True)\n",
        "    parser.add_argument('--gen_sem', default=True)\n",
        "    parser.add_argument('--gen_seq', default=True)\n",
        "    parser.add_argument('--dataset', type=str, default='mr')\n",
        "    parser.add_argument('--window_size', type=int, default=7)\n",
        "    parser.add_argument('--lr', type=float, default=1e-3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    parser.add_argument(\"--embed_size\", type=int, default=200)\n",
        "    parser.add_argument(\"--max_len\", default=512, type=int)\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=200)\n",
        "    parser.add_argument(\"--dropout\", default=0, type=float)\n",
        "    parser.add_argument(\"--weight_decay\", default=1e-6, type=float)\n",
        "    parser.add_argument(\"--epochs\", default=20, type=int)\n",
        "    parser.add_argument(\"--seed\", default=32, type=int)\n",
        "    parser.add_argument(\"--corenlp\", default='./stanford-corenlp-4.5.6')\n",
        "    parser.add_argument('--thres', default=0.05, type=float, help=\"the threshold of semantic graph\")\n",
        "    parser.add_argument('--num_layers', default=1, type=int, help=\"number of layers in LSTM\")\n",
        "    return parser.parse_known_args(args)[0]\n",
        "\n",
        "\n",
        "# retrieve execution timestamp for logs\n",
        "sgt = pytz.timezone('Asia/Singapore')\n",
        "timestamp = datetime.now(sgt).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# set up logging\n",
        "log_path = os.path.join(Path(os.path.abspath(os.path.dirname(\"__file__\")), '../logs'))\n",
        "logger = setup_logging(log_path=log_path, log_name='graph_log', log_filename='graph', timestamp=timestamp)\n",
        "\n",
        "main(parse_args(), timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W12MopV2f9al",
        "outputId": "c96b1832-0336-4cfc-fbd1-1eb4ec69af60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/dl\n"
          ]
        }
      ],
      "source": [
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muKk378fjYDR",
        "outputId": "8d084827-bd36-471e-9f27-5f2fe11a59a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R8_data.json  R8_lstm.bin  \u001b[0m\u001b[01;34mstanford-corenlp-4.5.6\u001b[0m/\n",
            "/content/gdrive/MyDrive/dl/stanford-corenlp-4.5.6\n"
          ]
        }
      ],
      "source": [
        "%ls\n",
        "%cd ./stanford-corenlp-4.5.6/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmhH4tUWjdY8",
        "outputId": "7af370b9-3e8a-498c-f971-55c8a5b9c99a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stanford-corenlp-4.5.6.jar\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gPn4AdynYqm"
      },
      "source": [
        "MODEL_PYTORCH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbETPofinSW6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TGCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_graphs, dropout=0.1, n_layers=2, bias=False, featureless=True, act='relu'):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = dropout\n",
        "        print(num_graphs)\n",
        "        self.embedding_list = nn.ModuleList([nn.Embedding(in_dim, hidden_dim) for _ in range(num_graphs)])\n",
        "        for embed in self.embedding_list:\n",
        "            nn.init.xavier_uniform_(embed.weight)\n",
        "        self.layers = nn.ModuleList([GraphConv_fix(in_dim=hidden_dim, out_dim=hidden_dim, num_graphs=num_graphs, dropout=dropout, featureless=True, bias=False, act=act)])\n",
        "        for _ in range(n_layers-2):\n",
        "            self.layers.append(GraphConv_fix(in_dim=hidden_dim, out_dim=hidden_dim, num_graphs=num_graphs, dropout=dropout, featureless=False, bias=False, act=act))\n",
        "        self.layers.append(GraphConv_fix(in_dim=hidden_dim, out_dim=out_dim, num_graphs=num_graphs, dropout=dropout, featureless=False, bias=False, act=act))\n",
        "        self.num_graphs = num_graphs\n",
        "\n",
        "    def word_dropout(self, inputs, keepprob):\n",
        "        features = [embed(inputs) for embed in self.embedding_list]\n",
        "        mask = torch.rand((features[0].size(0),1),device=features[0].device) > keepprob\n",
        "        features = [f.masked_fill(mask, 0)* (1.0/keepprob) for f in features]\n",
        "        return features\n",
        "\n",
        "    def forward(self, inputs, edge_indexs, edge_weights, keepprob):\n",
        "        features = self.word_dropout(inputs, keepprob)\n",
        "        for layer in self.layers:\n",
        "            features = layer(features, edge_indexs, edge_weights, keepprob)\n",
        "        features = torch.stack(features, dim=0)\n",
        "        features = torch.mean(features, dim=0)\n",
        "        return features\n",
        "\n",
        "class GraphConv_fix(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, num_graphs, dropout=0.1, featureless=False, bias=False, act='relu'):\n",
        "        super().__init__()\n",
        "        # net_dict = {'gcn':GCNConv, 'sage':SAGEConv, 'gat':GATConv}\n",
        "        # model_func = net_dict[kernel]\n",
        "        self.intra_convs = nn.ModuleList([GCNConv(in_dim, out_dim,add_self_loops=False,normalize=False) for _ in range(num_graphs)])\n",
        "\n",
        "\n",
        "        self.inter_convs = nn.ParameterList([nn.Parameter(torch.zeros((out_dim, out_dim), dtype=torch.float), requires_grad=True) for _ in range(num_graphs)])\n",
        "        for tmp in self.inter_convs:\n",
        "            nn.init.xavier_uniform_(tmp)\n",
        "        if act == 'relu':\n",
        "            self.act = nn.LeakyReLU(negative_slope=0.2)\n",
        "        else:\n",
        "            self.act = nn.Tanh()\n",
        "        self.bias = bias\n",
        "        # if self.bias:\n",
        "        #     self.bias = nn.Parameter(torch.zeros(out_dim), requires_grad=True)\n",
        "        #     nn.init.xavier_normal_(self.bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.featureless = featureless\n",
        "\n",
        "    def atten(self, supports):\n",
        "        tmp_supports = []\n",
        "\n",
        "        #0:Sequential Graph\n",
        "        #1:Semantic Graph\n",
        "        #2:Syntactic Graph\n",
        "        i = 2\n",
        "        supports[i] = torch.matmul(supports[i], self.inter_convs[i])\n",
        "        tmp_supports.append(supports[i])\n",
        "        tmp_supports = torch.stack(tmp_supports, dim=0)\n",
        "        tmp_supports_sum = torch.sum(tmp_supports, dim=0)\n",
        "        att_features = []\n",
        "        for support in supports:\n",
        "            att_features.append(self.act(tmp_supports_sum-support))\n",
        "\n",
        "        return att_features\n",
        "\n",
        "    def forward(self, inputs, edge_indexs, edge_weights, keepprob):\n",
        "        num_nodes = inputs[0].size(0)\n",
        "\n",
        "        if not self.featureless:\n",
        "            for i in range(len(inputs)):\n",
        "                inputs[i] = self.dropout(inputs[i])\n",
        "\n",
        "        supports = []\n",
        "        for i, conv in enumerate(self.intra_convs):\n",
        "            adj = SparseTensor(row=edge_indexs[i][0], col=edge_indexs[i][1], value=edge_weights[i],\n",
        "                   sparse_sizes=(num_nodes, num_nodes))\n",
        "            support = conv(inputs[i], adj.t())\n",
        "            support = self.act(support)\n",
        "            supports.append(support)\n",
        "\n",
        "        # Apply attention mechanism only to the first two graphs\n",
        "        supports = self.atten(supports)\n",
        "\n",
        "        self.embedding = torch.stack(supports, dim=0)\n",
        "        self.embedding = torch.mean(self.embedding, dim=0)\n",
        "\n",
        "        return supports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPO7l2-8d1N1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyhifOufniYK"
      },
      "source": [
        "UTIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N26dSMland_W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import scipy.sparse as sp\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import logging\n",
        "from logging.handlers import RotatingFileHandler\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    \"\"\"Parse index file.\"\"\"\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    \"\"\"Create mask.\"\"\"\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "def load_corpus_torch(args, device):\n",
        "    \"\"\"\n",
        "    Loads input corpus from gcn/data directory, torch tensor version\n",
        "\n",
        "    ind.dataset_str.x => the feature vectors of the training docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.tx => the feature vectors of the test docs as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training docs/words\n",
        "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.y => the one-hot labels of the labeled training docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ty => the one-hot labels of the test docs as numpy.ndarray object;\n",
        "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
        "    ind.dataset_str.adj => adjacency matrix of word/doc nodes as scipy.sparse.csr.csr_matrix object;\n",
        "    ind.dataset_str.train.index => the indices of training docs in original doc list.\n",
        "\n",
        "    All objects above must be saved using python pickle module.\n",
        "\n",
        "    :param dataset_str: Dataset name\n",
        "    :returns All data input files loaded (as well the training/test data).\n",
        "    Returns:\n",
        "        adj: sequential graph\n",
        "        adj1: semantic graph\n",
        "        adj2: syntactic graph\n",
        "    \"\"\"\n",
        "\n",
        "    adjs = []\n",
        "    for adj in ['seq','sem','syn']:\n",
        "        logger.info(\"Loading {} graph\".format(adj))\n",
        "        if args.run_id is not None:\n",
        "            try:\n",
        "                adjs.append(pkl.load(open('./saved_graphs/run_{}/{}.{}_adj'.format(args.run_id,args.dataset,adj),'rb')))\n",
        "                logger.info(\"Successfully loaded {} graph\".format(adj))\n",
        "            except Exception as e:\n",
        "                logger.info('Unable to locate run_{}/{}.{}_adj in the directory'.format(args.run_id,args.dataset,adj))\n",
        "        else:\n",
        "            adjs.append(pkl.load(open('./data/{}.{}_adj'.format(args.dataset,adj),'rb')))\n",
        "\n",
        "    data = json.load(open('./data/{}_data.json'.format(args.dataset),'r'))\n",
        "    train_ids, test_ids, corpus, labels, vocab, word_id_map, id_word_map, label_list = data\n",
        "\n",
        "    num_labels = len(label_list)\n",
        "    train_size = len(train_ids)\n",
        "\n",
        "    val_size = int(0.1*len(train_ids))\n",
        "    test_size = len(test_ids)\n",
        "\n",
        "    labels = np.asarray(labels[:train_size]+[0]*len(vocab)+labels[train_size:])\n",
        "    print(len(labels))\n",
        "\n",
        "\n",
        "    idx_train = range(train_size-val_size)\n",
        "    idx_val = range(train_size-val_size, train_size)\n",
        "    idx_test = range(train_size+len(vocab), train_size+len(vocab)+test_size)\n",
        "\n",
        "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
        "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "    y_train = np.zeros(labels.shape)\n",
        "    y_val = np.zeros(labels.shape)\n",
        "    y_test = np.zeros(labels.shape)\n",
        "    y_train[train_mask] = labels[train_mask]\n",
        "    y_val[val_mask] = labels[val_mask]\n",
        "    y_test[test_mask] = labels[test_mask]\n",
        "\n",
        "\n",
        "\n",
        "    # seq, sem, syn = adjs[0], adjs[1], adjs[2]\n",
        "    adjs_new = []\n",
        "    for adj in adjs:\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        adjs_new.append(adj)\n",
        "    # seq = seq + seq.T.multiply(seq.T > seq) - seq.multiply(seq.T > seq)\n",
        "    # sem = sem + sem.T.multiply(sem.T > sem) - sem.multiply(sem.T > sem)\n",
        "    # syn = syn + syn.T.multiply(syn.T > syn) - syn.multiply(syn.T > syn)\n",
        "\n",
        "    # tensor\n",
        "    # adj = torch.sparse_csr_tensor(adj.indptr, adj.indices, adj.data, dtype=torch.float).to_sparse_coo().to(device)\n",
        "    # adj1 = torch.sparse_csr_tensor(adj1.indptr, adj1.indices, adj1.data, dtype=torch.float).to_sparse_coo().to(device)\n",
        "    # adj2 = torch.sparse_csr_tensor(adj2.indptr, adj2.indices, adj2.data, dtype=torch.float).to_sparse_coo().to(device)\n",
        "    # features = torch.sparse_csr_tensor(features.indptr, features.indices, features.data, dtype=torch.float).to_sparse_coo().to(device)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "    train_mask = torch.tensor(train_mask, dtype=torch.float).to(device)\n",
        "    val_mask = torch.tensor(val_mask, dtype=torch.float).to(device)\n",
        "    test_mask = torch.tensor(test_mask, dtype=torch.float).to(device)\n",
        "\n",
        "    return adjs_new, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, val_size,test_size, num_labels\n",
        "\n",
        "def get_edge_tensor_list(adj_list, device):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        adj_list [list]: list of adjencies\n",
        "    \"\"\"\n",
        "    indice_list, data_list = [], []\n",
        "    for adj in adj_list:\n",
        "        row = torch.tensor(adj.row, dtype=torch.long).to(device)\n",
        "        col = torch.tensor(adj.col, dtype=torch.long).to(device)\n",
        "        data = torch.tensor(adj.data, dtype=torch.float).to(device)\n",
        "        indice = torch.stack((row,col),dim=0)\n",
        "        indice_list.append(indice)\n",
        "        data_list.append(data)\n",
        "    return indice_list, data_list\n",
        "\n",
        "def get_edge_tensor(adj):\n",
        "    row = torch.tensor(adj.row, dtype=torch.long)\n",
        "    col = torch.tensor(adj.col, dtype=torch.long)\n",
        "    data = torch.tensor(adj.data, dtype=torch.float)\n",
        "    indice = torch.stack((row,col),dim=0)\n",
        "    return indice, data\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "def preprocess_features(features):\n",
        "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "def preprocess_features_origin(features):\n",
        "    \"\"\"Row-normalize feature matrix\"\"\"\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "def preprocess_adj_mix(adj):\n",
        "    adj_normalized = adj + sp.eye(adj.shape[0])\n",
        "    return sparse_to_tuple(adj)\n",
        "\n",
        "def preprocess_adj_tensor(adj, device):\n",
        "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return torch.sparse_coo_tensor(np.stack([adj_normalized.row, adj_normalized.col], axis=0), adj_normalized.data, adj_normalized.shape, dtype=torch.float).to(device)\n",
        "\n",
        "def preprocess_adj_mix_tensor(adj, device):\n",
        "    adj_normalized = adj + sp.eye(adj.shape[0])\n",
        "    # return torch.sparse_csr_tensor(crow_indices=adj.indptr, col_indices=adj.indices, values=adj.data, dtype=torch.float).to_sparse_coo().to(device)\n",
        "    return torch.tensor(adj.todense(), dtype=torch.float).to(device)\n",
        "\n",
        "def pickle_graph(graph_type:str, dataset, graph_adj, graph_saved_path):\n",
        "    \"\"\"\n",
        "    Function to pickle graph using context manager\n",
        "\n",
        "    Args:\n",
        "        graph_type [str]: name of graph to be serialised\n",
        "        dataset [str]: name of dataset\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    start_time = time.time()\n",
        "    logger.info(f\"Persisting {graph_type} graph\")\n",
        "\n",
        "    with open(os.path.join(graph_saved_path, '{}.{}_adj').format(dataset, graph_type[:3]), 'wb') as f:\n",
        "        pkl.dump(graph_adj, f)\n",
        "    logger.info(\"Successfully persisted {} graph. Serialisation took {} seconds\".format(graph_type, round(time.time()-start_time),2))\n",
        "\n",
        "def set_torch_seed (seed:int=148):\n",
        "    \"\"\"\n",
        "    Function to randomly generate seed for torch to ensure reproducability\n",
        "\n",
        "    Args:\n",
        "        seed [int]: seed number\n",
        "    \"\"\"\n",
        "    seed=148\n",
        "    print(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def setup_logging(log_path, log_name, timestamp, log_filename='model', max_bytes=1048576, backup_count=3):\n",
        "    \"\"\"\n",
        "    Set up logging with RotatingFileHandler.\n",
        "\n",
        "    Args:\n",
        "    - log_path (str): Path to the directory where logs will be stored.\n",
        "    - log_filename (str): Base name of the log file (timestamp will be appended).\n",
        "    - max_bytes (int): Maximum size of each log file before it is rotated.\n",
        "    - backup_count (int): Number of backup log files to keep.\n",
        "    \"\"\"\n",
        "    # Create the log directory if it doesn't exist\n",
        "    if not os.path.exists(log_path):\n",
        "        os.makedirs(log_path)\n",
        "\n",
        "    # Set up logging\n",
        "    logger = logging.getLogger(log_name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Create a RotatingFileHandler\n",
        "    log_file = os.path.join(log_path, f\"{log_filename}_{timestamp}.log\")\n",
        "    handler = RotatingFileHandler(log_file, maxBytes=max_bytes, backupCount=backup_count)\n",
        "\n",
        "    # Create a formatter and set it for the handler\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "    # Add the handler to the logger\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "    # Create a StreamHandler to print logs to the terminal\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.DEBUG)  # Adjust log level if needed\n",
        "    console_handler.setFormatter(formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUu_t7Wo67J"
      },
      "source": [
        "main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm1UZJ3ovQ9C"
      },
      "outputs": [],
      "source": [
        "!pip install deepdish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T8hOpoGB16s"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZOU5OGKnka8",
        "outputId": "f54085d7-f203-46cc-eba9-2601715178c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 10:53:18,778 - INFO - Training is running on cuda\n",
            "INFO:training_log:Training is running on cuda\n",
            "2024-04-20 10:53:18,783 - INFO - Seed used: 147\n",
            "INFO:training_log:Seed used: 147\n",
            "2024-04-20 10:53:18,816 - INFO - Loading seq graph\n",
            "INFO:training_log:Loading seq graph\n",
            "2024-04-20 10:53:20,894 - INFO - Successfully loaded seq graph\n",
            "INFO:training_log:Successfully loaded seq graph\n",
            "2024-04-20 10:53:20,901 - INFO - Loading sem graph\n",
            "INFO:training_log:Loading sem graph\n",
            "2024-04-20 10:53:23,121 - INFO - Successfully loaded sem graph\n",
            "INFO:training_log:Successfully loaded sem graph\n",
            "2024-04-20 10:53:23,125 - INFO - Loading syn graph\n",
            "INFO:training_log:Loading syn graph\n",
            "2024-04-20 10:53:24,260 - INFO - Successfully loaded syn graph\n",
            "INFO:training_log:Successfully loaded syn graph\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-7eef63e03260>:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return np.array(mask, dtype=np.bool)\n",
            "2024-04-20 10:53:26,072 - DEBUG - adj:\n",
            "   (0, 7338)\t1.5008468300225124\n",
            "  (0, 7923)\t9.240961894450367\n",
            "  (0, 8680)\t3.1674184096404523\n",
            "  (0, 8878)\t2.764182956859557\n",
            "  (0, 11372)\t1.240134361043218\n",
            "  (0, 11457)\t1.9258537664551136\n",
            "  (0, 13795)\t3.3800384631178555\n",
            "  (0, 15527)\t4.997775178366651\n",
            "  (0, 16390)\t4.774631627052441\n",
            "  (0, 17380)\t6.016344759361225\n",
            "  (0, 18078)\t6.229918859659283\n",
            "  (0, 18216)\t6.709491939921169\n",
            "  (0, 18820)\t0.8349934545913218\n",
            "  (0, 20432)\t1.6673331910012603\n",
            "  (0, 20941)\t1.5195310253612764\n",
            "  (0, 21666)\t2.030213781779356\n",
            "  (0, 24561)\t4.983981856234315\n",
            "  (0, 24886)\t3.374543943800215\n",
            "  (0, 25530)\t6.229918859659283\n",
            "  (1, 10247)\t8.581294116822761\n",
            "  (1, 11651)\t1.0930021499776208\n",
            "  (1, 13323)\t8.175829008714597\n",
            "  (1, 17772)\t5.011761420341391\n",
            "  (1, 18265)\t7.665003384948606\n",
            "  (1, 18820)\t0.8349934545913218\n",
            "  :\t:\n",
            "  (29425, 13990)\t0.6764057181223682\n",
            "  (29425, 14028)\t4.126946820569254\n",
            "  (29425, 14275)\t6.789534647594706\n",
            "  (29425, 14667)\t6.330002318216266\n",
            "  (29425, 15159)\t5.808705394582979\n",
            "  (29425, 15622)\t2.717662941224664\n",
            "  (29425, 15624)\t2.090570582320254\n",
            "  (29425, 16833)\t7.482681828154651\n",
            "  (29425, 17009)\t4.855600689586108\n",
            "  (29425, 17118)\t4.774631627052441\n",
            "  (29425, 17518)\t8.175829008714597\n",
            "  (29425, 17553)\t2.1419437457226627\n",
            "  (29425, 18135)\t9.274441297382706\n",
            "  (29425, 18820)\t1.6699869091826436\n",
            "  (29425, 19249)\t0.7656835847875705\n",
            "  (29425, 19825)\t2.5047993205302035\n",
            "  (29425, 20120)\t7.888146936262816\n",
            "  (29425, 20432)\t0.5557777303337534\n",
            "  (29425, 21037)\t7.328531148327393\n",
            "  (29425, 21974)\t2.9718223216378012\n",
            "  (29425, 22509)\t8.581294116822761\n",
            "  (29425, 24926)\t3.258284137684353\n",
            "  (29425, 25090)\t5.3826209992720795\n",
            "  (29425, 25179)\t6.566391096280496\n",
            "  (29425, 25369)\t5.934084085826358\n",
            "DEBUG:training_log:adj:\n",
            "   (0, 7338)\t1.5008468300225124\n",
            "  (0, 7923)\t9.240961894450367\n",
            "  (0, 8680)\t3.1674184096404523\n",
            "  (0, 8878)\t2.764182956859557\n",
            "  (0, 11372)\t1.240134361043218\n",
            "  (0, 11457)\t1.9258537664551136\n",
            "  (0, 13795)\t3.3800384631178555\n",
            "  (0, 15527)\t4.997775178366651\n",
            "  (0, 16390)\t4.774631627052441\n",
            "  (0, 17380)\t6.016344759361225\n",
            "  (0, 18078)\t6.229918859659283\n",
            "  (0, 18216)\t6.709491939921169\n",
            "  (0, 18820)\t0.8349934545913218\n",
            "  (0, 20432)\t1.6673331910012603\n",
            "  (0, 20941)\t1.5195310253612764\n",
            "  (0, 21666)\t2.030213781779356\n",
            "  (0, 24561)\t4.983981856234315\n",
            "  (0, 24886)\t3.374543943800215\n",
            "  (0, 25530)\t6.229918859659283\n",
            "  (1, 10247)\t8.581294116822761\n",
            "  (1, 11651)\t1.0930021499776208\n",
            "  (1, 13323)\t8.175829008714597\n",
            "  (1, 17772)\t5.011761420341391\n",
            "  (1, 18265)\t7.665003384948606\n",
            "  (1, 18820)\t0.8349934545913218\n",
            "  :\t:\n",
            "  (29425, 13990)\t0.6764057181223682\n",
            "  (29425, 14028)\t4.126946820569254\n",
            "  (29425, 14275)\t6.789534647594706\n",
            "  (29425, 14667)\t6.330002318216266\n",
            "  (29425, 15159)\t5.808705394582979\n",
            "  (29425, 15622)\t2.717662941224664\n",
            "  (29425, 15624)\t2.090570582320254\n",
            "  (29425, 16833)\t7.482681828154651\n",
            "  (29425, 17009)\t4.855600689586108\n",
            "  (29425, 17118)\t4.774631627052441\n",
            "  (29425, 17518)\t8.175829008714597\n",
            "  (29425, 17553)\t2.1419437457226627\n",
            "  (29425, 18135)\t9.274441297382706\n",
            "  (29425, 18820)\t1.6699869091826436\n",
            "  (29425, 19249)\t0.7656835847875705\n",
            "  (29425, 19825)\t2.5047993205302035\n",
            "  (29425, 20120)\t7.888146936262816\n",
            "  (29425, 20432)\t0.5557777303337534\n",
            "  (29425, 21037)\t7.328531148327393\n",
            "  (29425, 21974)\t2.9718223216378012\n",
            "  (29425, 22509)\t8.581294116822761\n",
            "  (29425, 24926)\t3.258284137684353\n",
            "  (29425, 25090)\t5.3826209992720795\n",
            "  (29425, 25179)\t6.566391096280496\n",
            "  (29425, 25369)\t5.934084085826358\n",
            "2024-04-20 10:53:26,099 - INFO - The shape of adj is (29426, 29426)\n",
            "INFO:training_log:The shape of adj is (29426, 29426)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 10:53:27,534 - INFO - Starting training\n",
            "INFO:training_log:Starting training\n",
            "2024-04-20 10:53:31,334 - INFO - \n",
            " Epoch: 0001 train_loss= 1.97805 train_acc= 0.48625 val_loss= 0.80962 val_acc= 0.48169 test_loss= 0.80461 test_acc= 0.49212 time= 3.79053\n",
            "INFO:training_log:\n",
            " Epoch: 0001 train_loss= 1.97805 train_acc= 0.48625 val_loss= 0.80962 val_acc= 0.48169 test_loss= 0.80461 test_acc= 0.49212 time= 3.79053\n",
            "2024-04-20 10:53:31,355 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:31,359 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./saved_model/run_2024-04-20_18-53-18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-20 10:53:32,258 - INFO - Current best loss 0.80962\n",
            "INFO:training_log:Current best loss 0.80962\n",
            "2024-04-20 10:53:32,524 - INFO - \n",
            " Epoch: 0002 train_loss= 2.06954 train_acc= 0.46624 val_loss= 0.83376 val_acc= 0.48873 test_loss= 0.82129 test_acc= 0.49465 time= 0.25689\n",
            "INFO:training_log:\n",
            " Epoch: 0002 train_loss= 2.06954 train_acc= 0.46624 val_loss= 0.83376 val_acc= 0.48873 test_loss= 0.82129 test_acc= 0.49465 time= 0.25689\n",
            "2024-04-20 10:53:32,714 - INFO - \n",
            " Epoch: 0003 train_loss= 1.82477 train_acc= 0.51125 val_loss= 0.84399 val_acc= 0.48028 test_loss= 0.83005 test_acc= 0.49268 time= 0.18093\n",
            "INFO:training_log:\n",
            " Epoch: 0003 train_loss= 1.82477 train_acc= 0.51125 val_loss= 0.84399 val_acc= 0.48028 test_loss= 0.83005 test_acc= 0.49268 time= 0.18093\n",
            "2024-04-20 10:53:32,898 - INFO - \n",
            " Epoch: 0004 train_loss= 1.78542 train_acc= 0.50641 val_loss= 0.84126 val_acc= 0.49014 test_loss= 0.82738 test_acc= 0.49690 time= 0.17800\n",
            "INFO:training_log:\n",
            " Epoch: 0004 train_loss= 1.78542 train_acc= 0.50641 val_loss= 0.84126 val_acc= 0.49014 test_loss= 0.82738 test_acc= 0.49690 time= 0.17800\n",
            "2024-04-20 10:53:33,077 - INFO - \n",
            " Epoch: 0005 train_loss= 1.75704 train_acc= 0.51500 val_loss= 0.84103 val_acc= 0.49577 test_loss= 0.82704 test_acc= 0.49972 time= 0.17526\n",
            "INFO:training_log:\n",
            " Epoch: 0005 train_loss= 1.75704 train_acc= 0.51500 val_loss= 0.84103 val_acc= 0.49577 test_loss= 0.82704 test_acc= 0.49972 time= 0.17526\n",
            "2024-04-20 10:53:33,256 - INFO - \n",
            " Epoch: 0006 train_loss= 1.84731 train_acc= 0.50297 val_loss= 0.86560 val_acc= 0.49014 test_loss= 0.84977 test_acc= 0.50225 time= 0.17484\n",
            "INFO:training_log:\n",
            " Epoch: 0006 train_loss= 1.84731 train_acc= 0.50297 val_loss= 0.86560 val_acc= 0.49014 test_loss= 0.84977 test_acc= 0.50225 time= 0.17484\n",
            "2024-04-20 10:53:33,435 - INFO - \n",
            " Epoch: 0007 train_loss= 1.72161 train_acc= 0.51297 val_loss= 0.92456 val_acc= 0.49437 test_loss= 0.90607 test_acc= 0.50197 time= 0.17456\n",
            "INFO:training_log:\n",
            " Epoch: 0007 train_loss= 1.72161 train_acc= 0.51297 val_loss= 0.92456 val_acc= 0.49437 test_loss= 0.90607 test_acc= 0.50197 time= 0.17456\n",
            "2024-04-20 10:53:33,615 - INFO - \n",
            " Epoch: 0008 train_loss= 1.77229 train_acc= 0.52157 val_loss= 0.93890 val_acc= 0.49859 test_loss= 0.92004 test_acc= 0.50366 time= 0.17634\n",
            "INFO:training_log:\n",
            " Epoch: 0008 train_loss= 1.77229 train_acc= 0.52157 val_loss= 0.93890 val_acc= 0.49859 test_loss= 0.92004 test_acc= 0.50366 time= 0.17634\n",
            "2024-04-20 10:53:33,797 - INFO - \n",
            " Epoch: 0009 train_loss= 2.03425 train_acc= 0.50641 val_loss= 0.99902 val_acc= 0.49859 test_loss= 0.97866 test_acc= 0.50225 time= 0.17737\n",
            "INFO:training_log:\n",
            " Epoch: 0009 train_loss= 2.03425 train_acc= 0.50641 val_loss= 0.99902 val_acc= 0.49859 test_loss= 0.97866 test_acc= 0.50225 time= 0.17737\n",
            "2024-04-20 10:53:33,979 - INFO - \n",
            " Epoch: 0010 train_loss= 1.93252 train_acc= 0.49875 val_loss= 1.01312 val_acc= 0.49577 test_loss= 0.99276 test_acc= 0.50281 time= 0.17719\n",
            "INFO:training_log:\n",
            " Epoch: 0010 train_loss= 1.93252 train_acc= 0.49875 val_loss= 1.01312 val_acc= 0.49577 test_loss= 0.99276 test_acc= 0.50281 time= 0.17719\n",
            "2024-04-20 10:53:34,159 - INFO - \n",
            " Epoch: 0011 train_loss= 1.73891 train_acc= 0.52876 val_loss= 1.02011 val_acc= 0.49437 test_loss= 1.00006 test_acc= 0.50197 time= 0.17512\n",
            "INFO:training_log:\n",
            " Epoch: 0011 train_loss= 1.73891 train_acc= 0.52876 val_loss= 1.02011 val_acc= 0.49437 test_loss= 1.00006 test_acc= 0.50197 time= 0.17512\n",
            "2024-04-20 10:53:34,338 - INFO - \n",
            " Epoch: 0012 train_loss= 1.87738 train_acc= 0.50281 val_loss= 1.01475 val_acc= 0.49577 test_loss= 0.99522 test_acc= 0.50310 time= 0.17475\n",
            "INFO:training_log:\n",
            " Epoch: 0012 train_loss= 1.87738 train_acc= 0.50281 val_loss= 1.01475 val_acc= 0.49577 test_loss= 0.99522 test_acc= 0.50310 time= 0.17475\n",
            "2024-04-20 10:53:34,517 - INFO - \n",
            " Epoch: 0013 train_loss= 1.84644 train_acc= 0.49062 val_loss= 1.03346 val_acc= 0.49577 test_loss= 1.01378 test_acc= 0.50422 time= 0.17439\n",
            "INFO:training_log:\n",
            " Epoch: 0013 train_loss= 1.84644 train_acc= 0.49062 val_loss= 1.03346 val_acc= 0.49577 test_loss= 1.01378 test_acc= 0.50422 time= 0.17439\n",
            "2024-04-20 10:53:34,696 - INFO - \n",
            " Epoch: 0014 train_loss= 1.91318 train_acc= 0.50907 val_loss= 1.08208 val_acc= 0.49718 test_loss= 1.06167 test_acc= 0.50422 time= 0.17587\n",
            "INFO:training_log:\n",
            " Epoch: 0014 train_loss= 1.91318 train_acc= 0.50907 val_loss= 1.08208 val_acc= 0.49718 test_loss= 1.06167 test_acc= 0.50422 time= 0.17587\n",
            "2024-04-20 10:53:34,879 - INFO - \n",
            " Epoch: 0015 train_loss= 2.65585 train_acc= 0.51078 val_loss= 1.05800 val_acc= 0.49859 test_loss= 1.03813 test_acc= 0.50647 time= 0.17619\n",
            "INFO:training_log:\n",
            " Epoch: 0015 train_loss= 2.65585 train_acc= 0.51078 val_loss= 1.05800 val_acc= 0.49859 test_loss= 1.03813 test_acc= 0.50647 time= 0.17619\n",
            "2024-04-20 10:53:35,059 - INFO - \n",
            " Epoch: 0016 train_loss= 1.84444 train_acc= 0.50203 val_loss= 1.02005 val_acc= 0.50000 test_loss= 1.00099 test_acc= 0.50591 time= 0.17545\n",
            "INFO:training_log:\n",
            " Epoch: 0016 train_loss= 1.84444 train_acc= 0.50203 val_loss= 1.02005 val_acc= 0.50000 test_loss= 1.00099 test_acc= 0.50591 time= 0.17545\n",
            "2024-04-20 10:53:35,238 - INFO - \n",
            " Epoch: 0017 train_loss= 2.00666 train_acc= 0.50797 val_loss= 0.94394 val_acc= 0.50141 test_loss= 0.92660 test_acc= 0.52195 time= 0.17496\n",
            "INFO:training_log:\n",
            " Epoch: 0017 train_loss= 2.00666 train_acc= 0.50797 val_loss= 0.94394 val_acc= 0.50141 test_loss= 0.92660 test_acc= 0.52195 time= 0.17496\n",
            "2024-04-20 10:53:35,420 - INFO - \n",
            " Epoch: 0018 train_loss= 1.70818 train_acc= 0.50891 val_loss= 0.89966 val_acc= 0.50141 test_loss= 0.88354 test_acc= 0.52645 time= 0.17649\n",
            "INFO:training_log:\n",
            " Epoch: 0018 train_loss= 1.70818 train_acc= 0.50891 val_loss= 0.89966 val_acc= 0.50141 test_loss= 0.88354 test_acc= 0.52645 time= 0.17649\n",
            "2024-04-20 10:53:35,599 - INFO - \n",
            " Epoch: 0019 train_loss= 1.78377 train_acc= 0.51094 val_loss= 0.88477 val_acc= 0.50282 test_loss= 0.86920 test_acc= 0.52954 time= 0.17526\n",
            "INFO:training_log:\n",
            " Epoch: 0019 train_loss= 1.78377 train_acc= 0.51094 val_loss= 0.88477 val_acc= 0.50282 test_loss= 0.86920 test_acc= 0.52954 time= 0.17526\n",
            "2024-04-20 10:53:35,778 - INFO - \n",
            " Epoch: 0020 train_loss= 1.84088 train_acc= 0.49875 val_loss= 0.88218 val_acc= 0.50423 test_loss= 0.86672 test_acc= 0.53264 time= 0.17577\n",
            "INFO:training_log:\n",
            " Epoch: 0020 train_loss= 1.84088 train_acc= 0.49875 val_loss= 0.88218 val_acc= 0.50423 test_loss= 0.86672 test_acc= 0.53264 time= 0.17577\n",
            "2024-04-20 10:53:35,961 - INFO - \n",
            " Epoch: 0021 train_loss= 1.90056 train_acc= 0.48890 val_loss= 0.88200 val_acc= 0.50423 test_loss= 0.86651 test_acc= 0.53433 time= 0.17749\n",
            "INFO:training_log:\n",
            " Epoch: 0021 train_loss= 1.90056 train_acc= 0.48890 val_loss= 0.88200 val_acc= 0.50423 test_loss= 0.86651 test_acc= 0.53433 time= 0.17749\n",
            "2024-04-20 10:53:36,141 - INFO - \n",
            " Epoch: 0022 train_loss= 1.88857 train_acc= 0.52407 val_loss= 0.84981 val_acc= 0.51690 test_loss= 0.83509 test_acc= 0.54108 time= 0.17629\n",
            "INFO:training_log:\n",
            " Epoch: 0022 train_loss= 1.88857 train_acc= 0.52407 val_loss= 0.84981 val_acc= 0.51690 test_loss= 0.83509 test_acc= 0.54108 time= 0.17629\n",
            "2024-04-20 10:53:36,320 - INFO - \n",
            " Epoch: 0023 train_loss= 1.66357 train_acc= 0.52782 val_loss= 0.83297 val_acc= 0.52535 test_loss= 0.81867 test_acc= 0.54783 time= 0.17457\n",
            "INFO:training_log:\n",
            " Epoch: 0023 train_loss= 1.66357 train_acc= 0.52782 val_loss= 0.83297 val_acc= 0.52535 test_loss= 0.81867 test_acc= 0.54783 time= 0.17457\n",
            "2024-04-20 10:53:36,500 - INFO - \n",
            " Epoch: 0024 train_loss= 1.63994 train_acc= 0.52704 val_loss= 0.80926 val_acc= 0.53239 test_loss= 0.79565 test_acc= 0.55515 time= 0.17557\n",
            "INFO:training_log:\n",
            " Epoch: 0024 train_loss= 1.63994 train_acc= 0.52704 val_loss= 0.80926 val_acc= 0.53239 test_loss= 0.79565 test_acc= 0.55515 time= 0.17557\n",
            "2024-04-20 10:53:36,511 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:36,516 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:37,829 - INFO - Current best loss 0.80926\n",
            "INFO:training_log:Current best loss 0.80926\n",
            "2024-04-20 10:53:38,010 - INFO - \n",
            " Epoch: 0025 train_loss= 1.65821 train_acc= 0.51438 val_loss= 0.78936 val_acc= 0.53662 test_loss= 0.77629 test_acc= 0.55937 time= 0.17649\n",
            "INFO:training_log:\n",
            " Epoch: 0025 train_loss= 1.65821 train_acc= 0.51438 val_loss= 0.78936 val_acc= 0.53662 test_loss= 0.77629 test_acc= 0.55937 time= 0.17649\n",
            "2024-04-20 10:53:38,020 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:38,024 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:41,660 - INFO - Current best loss 0.78936\n",
            "INFO:training_log:Current best loss 0.78936\n",
            "2024-04-20 10:53:41,946 - INFO - \n",
            " Epoch: 0026 train_loss= 1.73339 train_acc= 0.51657 val_loss= 0.78668 val_acc= 0.53944 test_loss= 0.77348 test_acc= 0.56303 time= 0.28092\n",
            "INFO:training_log:\n",
            " Epoch: 0026 train_loss= 1.73339 train_acc= 0.51657 val_loss= 0.78668 val_acc= 0.53944 test_loss= 0.77348 test_acc= 0.56303 time= 0.28092\n",
            "2024-04-20 10:53:41,964 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:41,967 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:42,651 - INFO - Current best loss 0.78668\n",
            "INFO:training_log:Current best loss 0.78668\n",
            "2024-04-20 10:53:42,840 - INFO - \n",
            " Epoch: 0027 train_loss= 1.59775 train_acc= 0.54673 val_loss= 0.79459 val_acc= 0.54225 test_loss= 0.78094 test_acc= 0.56303 time= 0.18415\n",
            "INFO:training_log:\n",
            " Epoch: 0027 train_loss= 1.59775 train_acc= 0.54673 val_loss= 0.79459 val_acc= 0.54225 test_loss= 0.78094 test_acc= 0.56303 time= 0.18415\n",
            "2024-04-20 10:53:43,022 - INFO - \n",
            " Epoch: 0028 train_loss= 1.62631 train_acc= 0.53142 val_loss= 0.80785 val_acc= 0.54507 test_loss= 0.79359 test_acc= 0.56528 time= 0.17706\n",
            "INFO:training_log:\n",
            " Epoch: 0028 train_loss= 1.62631 train_acc= 0.53142 val_loss= 0.80785 val_acc= 0.54507 test_loss= 0.79359 test_acc= 0.56528 time= 0.17706\n",
            "2024-04-20 10:53:43,205 - INFO - \n",
            " Epoch: 0029 train_loss= 1.68200 train_acc= 0.53845 val_loss= 0.81228 val_acc= 0.54648 test_loss= 0.79784 test_acc= 0.56415 time= 0.17951\n",
            "INFO:training_log:\n",
            " Epoch: 0029 train_loss= 1.68200 train_acc= 0.53845 val_loss= 0.81228 val_acc= 0.54648 test_loss= 0.79784 test_acc= 0.56415 time= 0.17951\n",
            "2024-04-20 10:53:43,389 - INFO - \n",
            " Epoch: 0030 train_loss= 1.57023 train_acc= 0.52360 val_loss= 0.81388 val_acc= 0.54930 test_loss= 0.79936 test_acc= 0.56753 time= 0.17599\n",
            "INFO:training_log:\n",
            " Epoch: 0030 train_loss= 1.57023 train_acc= 0.52360 val_loss= 0.81388 val_acc= 0.54930 test_loss= 0.79936 test_acc= 0.56753 time= 0.17599\n",
            "2024-04-20 10:53:43,569 - INFO - \n",
            " Epoch: 0031 train_loss= 1.61796 train_acc= 0.51141 val_loss= 0.81863 val_acc= 0.55070 test_loss= 0.80399 test_acc= 0.56837 time= 0.17569\n",
            "INFO:training_log:\n",
            " Epoch: 0031 train_loss= 1.61796 train_acc= 0.51141 val_loss= 0.81863 val_acc= 0.55070 test_loss= 0.80399 test_acc= 0.56837 time= 0.17569\n",
            "2024-04-20 10:53:43,749 - INFO - \n",
            " Epoch: 0032 train_loss= 1.70723 train_acc= 0.52688 val_loss= 0.81628 val_acc= 0.55070 test_loss= 0.80169 test_acc= 0.57062 time= 0.17547\n",
            "INFO:training_log:\n",
            " Epoch: 0032 train_loss= 1.70723 train_acc= 0.52688 val_loss= 0.81628 val_acc= 0.55070 test_loss= 0.80169 test_acc= 0.57062 time= 0.17547\n",
            "2024-04-20 10:53:43,930 - INFO - \n",
            " Epoch: 0033 train_loss= 1.63967 train_acc= 0.52173 val_loss= 0.82794 val_acc= 0.54648 test_loss= 0.81303 test_acc= 0.56950 time= 0.17671\n",
            "INFO:training_log:\n",
            " Epoch: 0033 train_loss= 1.63967 train_acc= 0.52173 val_loss= 0.82794 val_acc= 0.54648 test_loss= 0.81303 test_acc= 0.56950 time= 0.17671\n",
            "2024-04-20 10:53:44,113 - INFO - \n",
            " Epoch: 0034 train_loss= 1.69536 train_acc= 0.53626 val_loss= 0.81303 val_acc= 0.54648 test_loss= 0.79841 test_acc= 0.57456 time= 0.17782\n",
            "INFO:training_log:\n",
            " Epoch: 0034 train_loss= 1.69536 train_acc= 0.53626 val_loss= 0.81303 val_acc= 0.54648 test_loss= 0.79841 test_acc= 0.57456 time= 0.17782\n",
            "2024-04-20 10:53:44,292 - INFO - \n",
            " Epoch: 0035 train_loss= 1.71706 train_acc= 0.51438 val_loss= 0.77664 val_acc= 0.56338 test_loss= 0.76298 test_acc= 0.58694 time= 0.17481\n",
            "INFO:training_log:\n",
            " Epoch: 0035 train_loss= 1.71706 train_acc= 0.51438 val_loss= 0.77664 val_acc= 0.56338 test_loss= 0.76298 test_acc= 0.58694 time= 0.17481\n",
            "2024-04-20 10:53:44,302 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:44,306 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:45,013 - INFO - Current best loss 0.77664\n",
            "INFO:training_log:Current best loss 0.77664\n",
            "2024-04-20 10:53:45,193 - INFO - \n",
            " Epoch: 0036 train_loss= 1.59493 train_acc= 0.54923 val_loss= 0.73621 val_acc= 0.58873 test_loss= 0.72404 test_acc= 0.59932 time= 0.17487\n",
            "INFO:training_log:\n",
            " Epoch: 0036 train_loss= 1.59493 train_acc= 0.54923 val_loss= 0.73621 val_acc= 0.58873 test_loss= 0.72404 test_acc= 0.59932 time= 0.17487\n",
            "2024-04-20 10:53:45,207 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:45,212 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:46,091 - INFO - Current best loss 0.73621\n",
            "INFO:training_log:Current best loss 0.73621\n",
            "2024-04-20 10:53:46,273 - INFO - \n",
            " Epoch: 0037 train_loss= 1.87915 train_acc= 0.53407 val_loss= 0.72049 val_acc= 0.60000 test_loss= 0.70913 test_acc= 0.60580 time= 0.17655\n",
            "INFO:training_log:\n",
            " Epoch: 0037 train_loss= 1.87915 train_acc= 0.53407 val_loss= 0.72049 val_acc= 0.60000 test_loss= 0.70913 test_acc= 0.60580 time= 0.17655\n",
            "2024-04-20 10:53:46,638 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:46,647 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:48,780 - INFO - Current best loss 0.72049\n",
            "INFO:training_log:Current best loss 0.72049\n",
            "2024-04-20 10:53:49,045 - INFO - \n",
            " Epoch: 0038 train_loss= 1.62621 train_acc= 0.52282 val_loss= 0.70840 val_acc= 0.61127 test_loss= 0.69765 test_acc= 0.61311 time= 0.25994\n",
            "INFO:training_log:\n",
            " Epoch: 0038 train_loss= 1.62621 train_acc= 0.52282 val_loss= 0.70840 val_acc= 0.61127 test_loss= 0.69765 test_acc= 0.61311 time= 0.25994\n",
            "2024-04-20 10:53:49,389 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:49,394 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:50,124 - INFO - Current best loss 0.70840\n",
            "INFO:training_log:Current best loss 0.70840\n",
            "2024-04-20 10:53:50,308 - INFO - \n",
            " Epoch: 0039 train_loss= 1.73199 train_acc= 0.53063 val_loss= 0.70956 val_acc= 0.61408 test_loss= 0.69865 test_acc= 0.61452 time= 0.17931\n",
            "INFO:training_log:\n",
            " Epoch: 0039 train_loss= 1.73199 train_acc= 0.53063 val_loss= 0.70956 val_acc= 0.61408 test_loss= 0.69865 test_acc= 0.61452 time= 0.17931\n",
            "2024-04-20 10:53:50,488 - INFO - \n",
            " Epoch: 0040 train_loss= 1.72795 train_acc= 0.52563 val_loss= 0.72019 val_acc= 0.60000 test_loss= 0.70870 test_acc= 0.61030 time= 0.17588\n",
            "INFO:training_log:\n",
            " Epoch: 0040 train_loss= 1.72795 train_acc= 0.52563 val_loss= 0.72019 val_acc= 0.60000 test_loss= 0.70870 test_acc= 0.61030 time= 0.17588\n",
            "2024-04-20 10:53:50,668 - INFO - \n",
            " Epoch: 0041 train_loss= 1.56514 train_acc= 0.54970 val_loss= 0.73426 val_acc= 0.59014 test_loss= 0.72223 test_acc= 0.60664 time= 0.17507\n",
            "INFO:training_log:\n",
            " Epoch: 0041 train_loss= 1.56514 train_acc= 0.54970 val_loss= 0.73426 val_acc= 0.59014 test_loss= 0.72223 test_acc= 0.60664 time= 0.17507\n",
            "2024-04-20 10:53:50,848 - INFO - \n",
            " Epoch: 0042 train_loss= 1.61520 train_acc= 0.54751 val_loss= 0.75570 val_acc= 0.58169 test_loss= 0.74307 test_acc= 0.60270 time= 0.17630\n",
            "INFO:training_log:\n",
            " Epoch: 0042 train_loss= 1.61520 train_acc= 0.54751 val_loss= 0.75570 val_acc= 0.58169 test_loss= 0.74307 test_acc= 0.60270 time= 0.17630\n",
            "2024-04-20 10:53:51,029 - INFO - \n",
            " Epoch: 0043 train_loss= 1.70124 train_acc= 0.53751 val_loss= 0.75973 val_acc= 0.57887 test_loss= 0.74706 test_acc= 0.60045 time= 0.17518\n",
            "INFO:training_log:\n",
            " Epoch: 0043 train_loss= 1.70124 train_acc= 0.53751 val_loss= 0.75973 val_acc= 0.57887 test_loss= 0.74706 test_acc= 0.60045 time= 0.17518\n",
            "2024-04-20 10:53:51,210 - INFO - \n",
            " Epoch: 0044 train_loss= 1.50300 train_acc= 0.53876 val_loss= 0.75951 val_acc= 0.58028 test_loss= 0.74695 test_acc= 0.59989 time= 0.17669\n",
            "INFO:training_log:\n",
            " Epoch: 0044 train_loss= 1.50300 train_acc= 0.53876 val_loss= 0.75951 val_acc= 0.58028 test_loss= 0.74695 test_acc= 0.59989 time= 0.17669\n",
            "2024-04-20 10:53:51,390 - INFO - \n",
            " Epoch: 0045 train_loss= 1.71348 train_acc= 0.51110 val_loss= 0.77701 val_acc= 0.57606 test_loss= 0.76417 test_acc= 0.59510 time= 0.17557\n",
            "INFO:training_log:\n",
            " Epoch: 0045 train_loss= 1.71348 train_acc= 0.51110 val_loss= 0.77701 val_acc= 0.57606 test_loss= 0.76417 test_acc= 0.59510 time= 0.17557\n",
            "2024-04-20 10:53:51,570 - INFO - \n",
            " Epoch: 0046 train_loss= 1.44760 train_acc= 0.55252 val_loss= 0.78753 val_acc= 0.58169 test_loss= 0.77458 test_acc= 0.59145 time= 0.17656\n",
            "INFO:training_log:\n",
            " Epoch: 0046 train_loss= 1.44760 train_acc= 0.55252 val_loss= 0.78753 val_acc= 0.58169 test_loss= 0.77458 test_acc= 0.59145 time= 0.17656\n",
            "2024-04-20 10:53:51,751 - INFO - \n",
            " Epoch: 0047 train_loss= 1.52359 train_acc= 0.54751 val_loss= 0.79148 val_acc= 0.58028 test_loss= 0.77854 test_acc= 0.59257 time= 0.17670\n",
            "INFO:training_log:\n",
            " Epoch: 0047 train_loss= 1.52359 train_acc= 0.54751 val_loss= 0.79148 val_acc= 0.58028 test_loss= 0.77854 test_acc= 0.59257 time= 0.17670\n",
            "2024-04-20 10:53:51,932 - INFO - \n",
            " Epoch: 0048 train_loss= 1.61210 train_acc= 0.54642 val_loss= 0.78020 val_acc= 0.58028 test_loss= 0.76775 test_acc= 0.59426 time= 0.17692\n",
            "INFO:training_log:\n",
            " Epoch: 0048 train_loss= 1.61210 train_acc= 0.54642 val_loss= 0.78020 val_acc= 0.58028 test_loss= 0.76775 test_acc= 0.59426 time= 0.17692\n",
            "2024-04-20 10:53:52,115 - INFO - \n",
            " Epoch: 0049 train_loss= 1.55546 train_acc= 0.54470 val_loss= 0.78724 val_acc= 0.58310 test_loss= 0.77480 test_acc= 0.59510 time= 0.17889\n",
            "INFO:training_log:\n",
            " Epoch: 0049 train_loss= 1.55546 train_acc= 0.54470 val_loss= 0.78724 val_acc= 0.58310 test_loss= 0.77480 test_acc= 0.59510 time= 0.17889\n",
            "2024-04-20 10:53:52,306 - INFO - \n",
            " Epoch: 0050 train_loss= 1.70858 train_acc= 0.51016 val_loss= 0.77898 val_acc= 0.58451 test_loss= 0.76691 test_acc= 0.59595 time= 0.18228\n",
            "INFO:training_log:\n",
            " Epoch: 0050 train_loss= 1.70858 train_acc= 0.51016 val_loss= 0.77898 val_acc= 0.58451 test_loss= 0.76691 test_acc= 0.59595 time= 0.18228\n",
            "2024-04-20 10:53:52,492 - INFO - \n",
            " Epoch: 0051 train_loss= 1.39863 train_acc= 0.56783 val_loss= 0.76695 val_acc= 0.58592 test_loss= 0.75529 test_acc= 0.60101 time= 0.18027\n",
            "INFO:training_log:\n",
            " Epoch: 0051 train_loss= 1.39863 train_acc= 0.56783 val_loss= 0.76695 val_acc= 0.58592 test_loss= 0.75529 test_acc= 0.60101 time= 0.18027\n",
            "2024-04-20 10:53:52,679 - INFO - \n",
            " Epoch: 0052 train_loss= 1.51649 train_acc= 0.55658 val_loss= 0.76679 val_acc= 0.58732 test_loss= 0.75518 test_acc= 0.60186 time= 0.17987\n",
            "INFO:training_log:\n",
            " Epoch: 0052 train_loss= 1.51649 train_acc= 0.55658 val_loss= 0.76679 val_acc= 0.58732 test_loss= 0.75518 test_acc= 0.60186 time= 0.17987\n",
            "2024-04-20 10:53:52,867 - INFO - \n",
            " Epoch: 0053 train_loss= 1.53526 train_acc= 0.55470 val_loss= 0.76156 val_acc= 0.59296 test_loss= 0.75010 test_acc= 0.60467 time= 0.18083\n",
            "INFO:training_log:\n",
            " Epoch: 0053 train_loss= 1.53526 train_acc= 0.55470 val_loss= 0.76156 val_acc= 0.59296 test_loss= 0.75010 test_acc= 0.60467 time= 0.18083\n",
            "2024-04-20 10:53:53,054 - INFO - \n",
            " Epoch: 0054 train_loss= 1.45659 train_acc= 0.55095 val_loss= 0.74778 val_acc= 0.59718 test_loss= 0.73665 test_acc= 0.61171 time= 0.17899\n",
            "INFO:training_log:\n",
            " Epoch: 0054 train_loss= 1.45659 train_acc= 0.55095 val_loss= 0.74778 val_acc= 0.59718 test_loss= 0.73665 test_acc= 0.61171 time= 0.17899\n",
            "2024-04-20 10:53:53,243 - INFO - \n",
            " Epoch: 0055 train_loss= 1.57847 train_acc= 0.53282 val_loss= 0.72295 val_acc= 0.62113 test_loss= 0.71257 test_acc= 0.62606 time= 0.18090\n",
            "INFO:training_log:\n",
            " Epoch: 0055 train_loss= 1.57847 train_acc= 0.53282 val_loss= 0.72295 val_acc= 0.62113 test_loss= 0.71257 test_acc= 0.62606 time= 0.18090\n",
            "2024-04-20 10:53:53,432 - INFO - \n",
            " Epoch: 0056 train_loss= 1.53268 train_acc= 0.56471 val_loss= 0.68969 val_acc= 0.64225 test_loss= 0.68061 test_acc= 0.64012 time= 0.18108\n",
            "INFO:training_log:\n",
            " Epoch: 0056 train_loss= 1.53268 train_acc= 0.56471 val_loss= 0.68969 val_acc= 0.64225 test_loss= 0.68061 test_acc= 0.64012 time= 0.18108\n",
            "2024-04-20 10:53:53,447 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:53,452 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:54,971 - INFO - Current best loss 0.68969\n",
            "INFO:training_log:Current best loss 0.68969\n",
            "2024-04-20 10:53:55,153 - INFO - \n",
            " Epoch: 0057 train_loss= 1.59674 train_acc= 0.53282 val_loss= 0.66217 val_acc= 0.65915 test_loss= 0.65466 test_acc= 0.65813 time= 0.17646\n",
            "INFO:training_log:\n",
            " Epoch: 0057 train_loss= 1.59674 train_acc= 0.53282 val_loss= 0.66217 val_acc= 0.65915 test_loss= 0.65466 test_acc= 0.65813 time= 0.17646\n",
            "2024-04-20 10:53:55,166 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:55,171 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:55,867 - INFO - Current best loss 0.66217\n",
            "INFO:training_log:Current best loss 0.66217\n",
            "2024-04-20 10:53:56,047 - INFO - \n",
            " Epoch: 0058 train_loss= 1.52119 train_acc= 0.55439 val_loss= 0.64325 val_acc= 0.67183 test_loss= 0.63755 test_acc= 0.66573 time= 0.17573\n",
            "INFO:training_log:\n",
            " Epoch: 0058 train_loss= 1.52119 train_acc= 0.55439 val_loss= 0.64325 val_acc= 0.67183 test_loss= 0.63755 test_acc= 0.66573 time= 0.17573\n",
            "2024-04-20 10:53:56,058 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:53:56,064 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:53:59,739 - INFO - Current best loss 0.64325\n",
            "INFO:training_log:Current best loss 0.64325\n",
            "2024-04-20 10:53:59,993 - INFO - \n",
            " Epoch: 0059 train_loss= 1.50566 train_acc= 0.54470 val_loss= 0.63463 val_acc= 0.65493 test_loss= 0.63128 test_acc= 0.65926 time= 0.24749\n",
            "INFO:training_log:\n",
            " Epoch: 0059 train_loss= 1.50566 train_acc= 0.54470 val_loss= 0.63463 val_acc= 0.65493 test_loss= 0.63128 test_acc= 0.65926 time= 0.24749\n",
            "2024-04-20 10:54:00,400 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:00,409 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:01,183 - INFO - Current best loss 0.63463\n",
            "INFO:training_log:Current best loss 0.63463\n",
            "2024-04-20 10:54:01,370 - INFO - \n",
            " Epoch: 0060 train_loss= 1.53768 train_acc= 0.54861 val_loss= 0.63742 val_acc= 0.64789 test_loss= 0.63611 test_acc= 0.65307 time= 0.18284\n",
            "INFO:training_log:\n",
            " Epoch: 0060 train_loss= 1.53768 train_acc= 0.54861 val_loss= 0.63742 val_acc= 0.64789 test_loss= 0.63611 test_acc= 0.65307 time= 0.18284\n",
            "2024-04-20 10:54:01,554 - INFO - \n",
            " Epoch: 0061 train_loss= 1.95896 train_acc= 0.52610 val_loss= 0.63857 val_acc= 0.65070 test_loss= 0.63790 test_acc= 0.65222 time= 0.17939\n",
            "INFO:training_log:\n",
            " Epoch: 0061 train_loss= 1.95896 train_acc= 0.52610 val_loss= 0.63857 val_acc= 0.65070 test_loss= 0.63790 test_acc= 0.65222 time= 0.17939\n",
            "2024-04-20 10:54:01,734 - INFO - \n",
            " Epoch: 0062 train_loss= 1.57115 train_acc= 0.53376 val_loss= 0.63604 val_acc= 0.65352 test_loss= 0.63521 test_acc= 0.65363 time= 0.17610\n",
            "INFO:training_log:\n",
            " Epoch: 0062 train_loss= 1.57115 train_acc= 0.53376 val_loss= 0.63604 val_acc= 0.65352 test_loss= 0.63521 test_acc= 0.65363 time= 0.17610\n",
            "2024-04-20 10:54:01,917 - INFO - \n",
            " Epoch: 0063 train_loss= 1.41297 train_acc= 0.56189 val_loss= 0.63189 val_acc= 0.65070 test_loss= 0.63038 test_acc= 0.65954 time= 0.17665\n",
            "INFO:training_log:\n",
            " Epoch: 0063 train_loss= 1.41297 train_acc= 0.56189 val_loss= 0.63189 val_acc= 0.65070 test_loss= 0.63038 test_acc= 0.65954 time= 0.17665\n",
            "2024-04-20 10:54:01,929 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:01,933 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:02,888 - INFO - Current best loss 0.63189\n",
            "INFO:training_log:Current best loss 0.63189\n",
            "2024-04-20 10:54:03,077 - INFO - \n",
            " Epoch: 0064 train_loss= 1.61105 train_acc= 0.54486 val_loss= 0.62826 val_acc= 0.65352 test_loss= 0.62531 test_acc= 0.66545 time= 0.18148\n",
            "INFO:training_log:\n",
            " Epoch: 0064 train_loss= 1.61105 train_acc= 0.54486 val_loss= 0.62826 val_acc= 0.65352 test_loss= 0.62531 test_acc= 0.66545 time= 0.18148\n",
            "2024-04-20 10:54:03,088 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:03,093 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:03,888 - INFO - Current best loss 0.62826\n",
            "INFO:training_log:Current best loss 0.62826\n",
            "2024-04-20 10:54:04,080 - INFO - \n",
            " Epoch: 0065 train_loss= 1.56138 train_acc= 0.56127 val_loss= 0.62712 val_acc= 0.66338 test_loss= 0.62365 test_acc= 0.66995 time= 0.18673\n",
            "INFO:training_log:\n",
            " Epoch: 0065 train_loss= 1.56138 train_acc= 0.56127 val_loss= 0.62712 val_acc= 0.66338 test_loss= 0.62365 test_acc= 0.66995 time= 0.18673\n",
            "2024-04-20 10:54:04,108 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:04,119 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:04,904 - INFO - Current best loss 0.62712\n",
            "INFO:training_log:Current best loss 0.62712\n",
            "2024-04-20 10:54:05,094 - INFO - \n",
            " Epoch: 0066 train_loss= 1.59410 train_acc= 0.56330 val_loss= 0.62586 val_acc= 0.65634 test_loss= 0.62274 test_acc= 0.66826 time= 0.18401\n",
            "INFO:training_log:\n",
            " Epoch: 0066 train_loss= 1.59410 train_acc= 0.56330 val_loss= 0.62586 val_acc= 0.65634 test_loss= 0.62274 test_acc= 0.66826 time= 0.18401\n",
            "2024-04-20 10:54:05,105 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:05,114 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:08,102 - INFO - Current best loss 0.62586\n",
            "INFO:training_log:Current best loss 0.62586\n",
            "2024-04-20 10:54:08,362 - INFO - \n",
            " Epoch: 0067 train_loss= 1.37165 train_acc= 0.58550 val_loss= 0.62471 val_acc= 0.65634 test_loss= 0.62192 test_acc= 0.66995 time= 0.25591\n",
            "INFO:training_log:\n",
            " Epoch: 0067 train_loss= 1.37165 train_acc= 0.58550 val_loss= 0.62471 val_acc= 0.65634 test_loss= 0.62192 test_acc= 0.66995 time= 0.25591\n",
            "2024-04-20 10:54:08,838 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:08,842 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:09,556 - INFO - Current best loss 0.62471\n",
            "INFO:training_log:Current best loss 0.62471\n",
            "2024-04-20 10:54:09,743 - INFO - \n",
            " Epoch: 0068 train_loss= 1.57156 train_acc= 0.54392 val_loss= 0.62367 val_acc= 0.66056 test_loss= 0.62035 test_acc= 0.67361 time= 0.17971\n",
            "INFO:training_log:\n",
            " Epoch: 0068 train_loss= 1.57156 train_acc= 0.54392 val_loss= 0.62367 val_acc= 0.66056 test_loss= 0.62035 test_acc= 0.67361 time= 0.17971\n",
            "2024-04-20 10:54:09,759 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:09,764 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:13,009 - INFO - Current best loss 0.62367\n",
            "INFO:training_log:Current best loss 0.62367\n",
            "2024-04-20 10:54:13,267 - INFO - \n",
            " Epoch: 0069 train_loss= 1.49115 train_acc= 0.57158 val_loss= 0.62609 val_acc= 0.67887 test_loss= 0.62145 test_acc= 0.67586 time= 0.24915\n",
            "INFO:training_log:\n",
            " Epoch: 0069 train_loss= 1.49115 train_acc= 0.57158 val_loss= 0.62609 val_acc= 0.67887 test_loss= 0.62145 test_acc= 0.67586 time= 0.24915\n",
            "2024-04-20 10:54:13,465 - INFO - \n",
            " Epoch: 0070 train_loss= 1.99291 train_acc= 0.54189 val_loss= 0.64284 val_acc= 0.67887 test_loss= 0.63600 test_acc= 0.67586 time= 0.19328\n",
            "INFO:training_log:\n",
            " Epoch: 0070 train_loss= 1.99291 train_acc= 0.54189 val_loss= 0.64284 val_acc= 0.67887 test_loss= 0.63600 test_acc= 0.67586 time= 0.19328\n",
            "2024-04-20 10:54:13,646 - INFO - \n",
            " Epoch: 0071 train_loss= 1.92997 train_acc= 0.54298 val_loss= 0.68562 val_acc= 0.64789 test_loss= 0.67645 test_acc= 0.65926 time= 0.17671\n",
            "INFO:training_log:\n",
            " Epoch: 0071 train_loss= 1.92997 train_acc= 0.54298 val_loss= 0.68562 val_acc= 0.64789 test_loss= 0.67645 test_acc= 0.65926 time= 0.17671\n",
            "2024-04-20 10:54:13,828 - INFO - \n",
            " Epoch: 0072 train_loss= 1.44447 train_acc= 0.54939 val_loss= 0.73796 val_acc= 0.63944 test_loss= 0.72726 test_acc= 0.63112 time= 0.17742\n",
            "INFO:training_log:\n",
            " Epoch: 0072 train_loss= 1.44447 train_acc= 0.54939 val_loss= 0.73796 val_acc= 0.63944 test_loss= 0.72726 test_acc= 0.63112 time= 0.17742\n",
            "2024-04-20 10:54:14,009 - INFO - \n",
            " Epoch: 0073 train_loss= 1.43545 train_acc= 0.54439 val_loss= 0.79638 val_acc= 0.61127 test_loss= 0.78459 test_acc= 0.61564 time= 0.17710\n",
            "INFO:training_log:\n",
            " Epoch: 0073 train_loss= 1.43545 train_acc= 0.54439 val_loss= 0.79638 val_acc= 0.61127 test_loss= 0.78459 test_acc= 0.61564 time= 0.17710\n",
            "2024-04-20 10:54:14,190 - INFO - \n",
            " Epoch: 0074 train_loss= 1.58144 train_acc= 0.56018 val_loss= 0.83417 val_acc= 0.59859 test_loss= 0.82188 test_acc= 0.60833 time= 0.17701\n",
            "INFO:training_log:\n",
            " Epoch: 0074 train_loss= 1.58144 train_acc= 0.56018 val_loss= 0.83417 val_acc= 0.59859 test_loss= 0.82188 test_acc= 0.60833 time= 0.17701\n",
            "2024-04-20 10:54:14,373 - INFO - \n",
            " Epoch: 0075 train_loss= 1.76384 train_acc= 0.55611 val_loss= 0.83864 val_acc= 0.60000 test_loss= 0.82636 test_acc= 0.60805 time= 0.17860\n",
            "INFO:training_log:\n",
            " Epoch: 0075 train_loss= 1.76384 train_acc= 0.55611 val_loss= 0.83864 val_acc= 0.60000 test_loss= 0.82636 test_acc= 0.60805 time= 0.17860\n",
            "2024-04-20 10:54:14,556 - INFO - \n",
            " Epoch: 0076 train_loss= 1.42732 train_acc= 0.56674 val_loss= 0.83610 val_acc= 0.60423 test_loss= 0.82394 test_acc= 0.60917 time= 0.17714\n",
            "INFO:training_log:\n",
            " Epoch: 0076 train_loss= 1.42732 train_acc= 0.56674 val_loss= 0.83610 val_acc= 0.60423 test_loss= 0.82394 test_acc= 0.60917 time= 0.17714\n",
            "2024-04-20 10:54:14,738 - INFO - \n",
            " Epoch: 0077 train_loss= 1.62908 train_acc= 0.56987 val_loss= 0.80802 val_acc= 0.60986 test_loss= 0.79631 test_acc= 0.61508 time= 0.17771\n",
            "INFO:training_log:\n",
            " Epoch: 0077 train_loss= 1.62908 train_acc= 0.56987 val_loss= 0.80802 val_acc= 0.60986 test_loss= 0.79631 test_acc= 0.61508 time= 0.17771\n",
            "2024-04-20 10:54:14,920 - INFO - \n",
            " Epoch: 0078 train_loss= 1.49664 train_acc= 0.58440 val_loss= 0.77102 val_acc= 0.63662 test_loss= 0.75996 test_acc= 0.62662 time= 0.17849\n",
            "INFO:training_log:\n",
            " Epoch: 0078 train_loss= 1.49664 train_acc= 0.58440 val_loss= 0.77102 val_acc= 0.63662 test_loss= 0.75996 test_acc= 0.62662 time= 0.17849\n",
            "2024-04-20 10:54:15,100 - INFO - \n",
            " Epoch: 0079 train_loss= 1.36758 train_acc= 0.58002 val_loss= 0.74799 val_acc= 0.64085 test_loss= 0.73744 test_acc= 0.63675 time= 0.17616\n",
            "INFO:training_log:\n",
            " Epoch: 0079 train_loss= 1.36758 train_acc= 0.58002 val_loss= 0.74799 val_acc= 0.64085 test_loss= 0.73744 test_acc= 0.63675 time= 0.17616\n",
            "2024-04-20 10:54:15,281 - INFO - \n",
            " Epoch: 0080 train_loss= 1.38401 train_acc= 0.56205 val_loss= 0.71947 val_acc= 0.64930 test_loss= 0.70965 test_acc= 0.64913 time= 0.17591\n",
            "INFO:training_log:\n",
            " Epoch: 0080 train_loss= 1.38401 train_acc= 0.56205 val_loss= 0.71947 val_acc= 0.64930 test_loss= 0.70965 test_acc= 0.64913 time= 0.17591\n",
            "2024-04-20 10:54:15,461 - INFO - \n",
            " Epoch: 0081 train_loss= 1.52432 train_acc= 0.56674 val_loss= 0.68135 val_acc= 0.66056 test_loss= 0.67264 test_acc= 0.66376 time= 0.17660\n",
            "INFO:training_log:\n",
            " Epoch: 0081 train_loss= 1.52432 train_acc= 0.56674 val_loss= 0.68135 val_acc= 0.66056 test_loss= 0.67264 test_acc= 0.66376 time= 0.17660\n",
            "2024-04-20 10:54:15,644 - INFO - \n",
            " Epoch: 0082 train_loss= 1.41515 train_acc= 0.56471 val_loss= 0.64878 val_acc= 0.67465 test_loss= 0.64149 test_acc= 0.67923 time= 0.17699\n",
            "INFO:training_log:\n",
            " Epoch: 0082 train_loss= 1.41515 train_acc= 0.56471 val_loss= 0.64878 val_acc= 0.67465 test_loss= 0.64149 test_acc= 0.67923 time= 0.17699\n",
            "2024-04-20 10:54:15,824 - INFO - \n",
            " Epoch: 0083 train_loss= 1.37881 train_acc= 0.58128 val_loss= 0.62887 val_acc= 0.69577 test_loss= 0.62286 test_acc= 0.68458 time= 0.17661\n",
            "INFO:training_log:\n",
            " Epoch: 0083 train_loss= 1.37881 train_acc= 0.58128 val_loss= 0.62887 val_acc= 0.69577 test_loss= 0.62286 test_acc= 0.68458 time= 0.17661\n",
            "2024-04-20 10:54:16,006 - INFO - \n",
            " Epoch: 0084 train_loss= 1.33792 train_acc= 0.58737 val_loss= 0.61895 val_acc= 0.69437 test_loss= 0.61382 test_acc= 0.68880 time= 0.17738\n",
            "INFO:training_log:\n",
            " Epoch: 0084 train_loss= 1.33792 train_acc= 0.58737 val_loss= 0.61895 val_acc= 0.69437 test_loss= 0.61382 test_acc= 0.68880 time= 0.17738\n",
            "2024-04-20 10:54:16,018 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:16,022 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:16,713 - INFO - Current best loss 0.61895\n",
            "INFO:training_log:Current best loss 0.61895\n",
            "2024-04-20 10:54:16,896 - INFO - \n",
            " Epoch: 0085 train_loss= 1.50434 train_acc= 0.55502 val_loss= 0.61257 val_acc= 0.68451 test_loss= 0.60824 test_acc= 0.68683 time= 0.17651\n",
            "INFO:training_log:\n",
            " Epoch: 0085 train_loss= 1.50434 train_acc= 0.55502 val_loss= 0.61257 val_acc= 0.68451 test_loss= 0.60824 test_acc= 0.68683 time= 0.17651\n",
            "2024-04-20 10:54:16,908 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:16,912 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:20,344 - INFO - Current best loss 0.61257\n",
            "INFO:training_log:Current best loss 0.61257\n",
            "2024-04-20 10:54:20,616 - INFO - \n",
            " Epoch: 0086 train_loss= 1.41720 train_acc= 0.59237 val_loss= 0.61094 val_acc= 0.68732 test_loss= 0.60679 test_acc= 0.68880 time= 0.26654\n",
            "INFO:training_log:\n",
            " Epoch: 0086 train_loss= 1.41720 train_acc= 0.59237 val_loss= 0.61094 val_acc= 0.68732 test_loss= 0.60679 test_acc= 0.68880 time= 0.26654\n",
            "2024-04-20 10:54:21,084 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:21,089 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:21,782 - INFO - Current best loss 0.61094\n",
            "INFO:training_log:Current best loss 0.61094\n",
            "2024-04-20 10:54:21,966 - INFO - \n",
            " Epoch: 0087 train_loss= 1.48148 train_acc= 0.55095 val_loss= 0.60796 val_acc= 0.68732 test_loss= 0.60453 test_acc= 0.69077 time= 0.17928\n",
            "INFO:training_log:\n",
            " Epoch: 0087 train_loss= 1.48148 train_acc= 0.55095 val_loss= 0.60796 val_acc= 0.68732 test_loss= 0.60453 test_acc= 0.69077 time= 0.17928\n",
            "2024-04-20 10:54:21,979 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:21,983 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:25,355 - INFO - Current best loss 0.60796\n",
            "INFO:training_log:Current best loss 0.60796\n",
            "2024-04-20 10:54:25,630 - INFO - \n",
            " Epoch: 0088 train_loss= 1.39345 train_acc= 0.55252 val_loss= 0.60665 val_acc= 0.68028 test_loss= 0.60365 test_acc= 0.69105 time= 0.26808\n",
            "INFO:training_log:\n",
            " Epoch: 0088 train_loss= 1.39345 train_acc= 0.55252 val_loss= 0.60665 val_acc= 0.68028 test_loss= 0.60365 test_acc= 0.69105 time= 0.26808\n",
            "2024-04-20 10:54:25,642 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:25,645 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:26,441 - INFO - Current best loss 0.60665\n",
            "INFO:training_log:Current best loss 0.60665\n",
            "2024-04-20 10:54:26,624 - INFO - \n",
            " Epoch: 0089 train_loss= 1.43806 train_acc= 0.55392 val_loss= 0.60624 val_acc= 0.67746 test_loss= 0.60374 test_acc= 0.68824 time= 0.17892\n",
            "INFO:training_log:\n",
            " Epoch: 0089 train_loss= 1.43806 train_acc= 0.55392 val_loss= 0.60624 val_acc= 0.67746 test_loss= 0.60374 test_acc= 0.68824 time= 0.17892\n",
            "2024-04-20 10:54:27,280 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:27,284 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:27,975 - INFO - Current best loss 0.60624\n",
            "INFO:training_log:Current best loss 0.60624\n",
            "2024-04-20 10:54:28,159 - INFO - \n",
            " Epoch: 0090 train_loss= 1.58304 train_acc= 0.57315 val_loss= 0.60545 val_acc= 0.67746 test_loss= 0.60255 test_acc= 0.68824 time= 0.17895\n",
            "INFO:training_log:\n",
            " Epoch: 0090 train_loss= 1.58304 train_acc= 0.57315 val_loss= 0.60545 val_acc= 0.67746 test_loss= 0.60255 test_acc= 0.68824 time= 0.17895\n",
            "2024-04-20 10:54:28,173 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:28,178 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:28,863 - INFO - Current best loss 0.60545\n",
            "INFO:training_log:Current best loss 0.60545\n",
            "2024-04-20 10:54:29,047 - INFO - \n",
            " Epoch: 0091 train_loss= 1.32079 train_acc= 0.58268 val_loss= 0.60544 val_acc= 0.69296 test_loss= 0.60175 test_acc= 0.69302 time= 0.17920\n",
            "INFO:training_log:\n",
            " Epoch: 0091 train_loss= 1.32079 train_acc= 0.58268 val_loss= 0.60544 val_acc= 0.69296 test_loss= 0.60175 test_acc= 0.69302 time= 0.17920\n",
            "2024-04-20 10:54:29,060 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:29,064 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:32,017 - INFO - Current best loss 0.60544\n",
            "INFO:training_log:Current best loss 0.60544\n",
            "2024-04-20 10:54:32,281 - INFO - \n",
            " Epoch: 0092 train_loss= 1.51605 train_acc= 0.56049 val_loss= 0.60895 val_acc= 0.69296 test_loss= 0.60409 test_acc= 0.69246 time= 0.25452\n",
            "INFO:training_log:\n",
            " Epoch: 0092 train_loss= 1.51605 train_acc= 0.56049 val_loss= 0.60895 val_acc= 0.69296 test_loss= 0.60409 test_acc= 0.69246 time= 0.25452\n",
            "2024-04-20 10:54:32,467 - INFO - \n",
            " Epoch: 0093 train_loss= 1.33878 train_acc= 0.57237 val_loss= 0.61827 val_acc= 0.69437 test_loss= 0.61212 test_acc= 0.69274 time= 0.18232\n",
            "INFO:training_log:\n",
            " Epoch: 0093 train_loss= 1.33878 train_acc= 0.57237 val_loss= 0.61827 val_acc= 0.69437 test_loss= 0.61212 test_acc= 0.69274 time= 0.18232\n",
            "2024-04-20 10:54:32,665 - INFO - \n",
            " Epoch: 0094 train_loss= 1.29911 train_acc= 0.57768 val_loss= 0.62660 val_acc= 0.70563 test_loss= 0.61968 test_acc= 0.68936 time= 0.19244\n",
            "INFO:training_log:\n",
            " Epoch: 0094 train_loss= 1.29911 train_acc= 0.57768 val_loss= 0.62660 val_acc= 0.70563 test_loss= 0.61968 test_acc= 0.68936 time= 0.19244\n",
            "2024-04-20 10:54:32,850 - INFO - \n",
            " Epoch: 0095 train_loss= 1.38033 train_acc= 0.57237 val_loss= 0.63711 val_acc= 0.69718 test_loss= 0.62944 test_acc= 0.68824 time= 0.17811\n",
            "INFO:training_log:\n",
            " Epoch: 0095 train_loss= 1.38033 train_acc= 0.57237 val_loss= 0.63711 val_acc= 0.69718 test_loss= 0.62944 test_acc= 0.68824 time= 0.17811\n",
            "2024-04-20 10:54:33,030 - INFO - \n",
            " Epoch: 0096 train_loss= 1.35826 train_acc= 0.57956 val_loss= 0.64513 val_acc= 0.69437 test_loss= 0.63699 test_acc= 0.68852 time= 0.17590\n",
            "INFO:training_log:\n",
            " Epoch: 0096 train_loss= 1.35826 train_acc= 0.57956 val_loss= 0.64513 val_acc= 0.69437 test_loss= 0.63699 test_acc= 0.68852 time= 0.17590\n",
            "2024-04-20 10:54:33,211 - INFO - \n",
            " Epoch: 0097 train_loss= 1.38176 train_acc= 0.57815 val_loss= 0.66083 val_acc= 0.68310 test_loss= 0.65201 test_acc= 0.68120 time= 0.17690\n",
            "INFO:training_log:\n",
            " Epoch: 0097 train_loss= 1.38176 train_acc= 0.57815 val_loss= 0.66083 val_acc= 0.68310 test_loss= 0.65201 test_acc= 0.68120 time= 0.17690\n",
            "2024-04-20 10:54:33,395 - INFO - \n",
            " Epoch: 0098 train_loss= 1.58836 train_acc= 0.55002 val_loss= 0.69064 val_acc= 0.67465 test_loss= 0.68085 test_acc= 0.66545 time= 0.17971\n",
            "INFO:training_log:\n",
            " Epoch: 0098 train_loss= 1.58836 train_acc= 0.55002 val_loss= 0.69064 val_acc= 0.67465 test_loss= 0.68085 test_acc= 0.66545 time= 0.17971\n",
            "2024-04-20 10:54:33,577 - INFO - \n",
            " Epoch: 0099 train_loss= 1.88341 train_acc= 0.54751 val_loss= 0.69684 val_acc= 0.66761 test_loss= 0.68688 test_acc= 0.66545 time= 0.17821\n",
            "INFO:training_log:\n",
            " Epoch: 0099 train_loss= 1.88341 train_acc= 0.54751 val_loss= 0.69684 val_acc= 0.66761 test_loss= 0.68688 test_acc= 0.66545 time= 0.17821\n",
            "2024-04-20 10:54:33,759 - INFO - \n",
            " Epoch: 0100 train_loss= 1.33911 train_acc= 0.57237 val_loss= 0.70932 val_acc= 0.66197 test_loss= 0.69905 test_acc= 0.66123 time= 0.17846\n",
            "INFO:training_log:\n",
            " Epoch: 0100 train_loss= 1.33911 train_acc= 0.57237 val_loss= 0.70932 val_acc= 0.66197 test_loss= 0.69905 test_acc= 0.66123 time= 0.17846\n",
            "2024-04-20 10:54:33,941 - INFO - \n",
            " Epoch: 0101 train_loss= 1.35217 train_acc= 0.57190 val_loss= 0.71723 val_acc= 0.66338 test_loss= 0.70681 test_acc= 0.65898 time= 0.17725\n",
            "INFO:training_log:\n",
            " Epoch: 0101 train_loss= 1.35217 train_acc= 0.57190 val_loss= 0.71723 val_acc= 0.66338 test_loss= 0.70681 test_acc= 0.65898 time= 0.17725\n",
            "2024-04-20 10:54:34,123 - INFO - \n",
            " Epoch: 0102 train_loss= 1.77830 train_acc= 0.54877 val_loss= 0.70139 val_acc= 0.66761 test_loss= 0.69144 test_acc= 0.66488 time= 0.17833\n",
            "INFO:training_log:\n",
            " Epoch: 0102 train_loss= 1.77830 train_acc= 0.54877 val_loss= 0.70139 val_acc= 0.66761 test_loss= 0.69144 test_acc= 0.66488 time= 0.17833\n",
            "2024-04-20 10:54:34,305 - INFO - \n",
            " Epoch: 0103 train_loss= 1.49828 train_acc= 0.56471 val_loss= 0.67586 val_acc= 0.69296 test_loss= 0.66679 test_acc= 0.67614 time= 0.17774\n",
            "INFO:training_log:\n",
            " Epoch: 0103 train_loss= 1.49828 train_acc= 0.56471 val_loss= 0.67586 val_acc= 0.69296 test_loss= 0.66679 test_acc= 0.67614 time= 0.17774\n",
            "2024-04-20 10:54:34,487 - INFO - \n",
            " Epoch: 0104 train_loss= 1.48109 train_acc= 0.56877 val_loss= 0.66737 val_acc= 0.69014 test_loss= 0.65868 test_acc= 0.67923 time= 0.17724\n",
            "INFO:training_log:\n",
            " Epoch: 0104 train_loss= 1.48109 train_acc= 0.56877 val_loss= 0.66737 val_acc= 0.69014 test_loss= 0.65868 test_acc= 0.67923 time= 0.17724\n",
            "2024-04-20 10:54:34,670 - INFO - \n",
            " Epoch: 0105 train_loss= 1.32696 train_acc= 0.56830 val_loss= 0.66350 val_acc= 0.69437 test_loss= 0.65514 test_acc= 0.68149 time= 0.17857\n",
            "INFO:training_log:\n",
            " Epoch: 0105 train_loss= 1.32696 train_acc= 0.56830 val_loss= 0.66350 val_acc= 0.69437 test_loss= 0.65514 test_acc= 0.68149 time= 0.17857\n",
            "2024-04-20 10:54:34,853 - INFO - \n",
            " Epoch: 0106 train_loss= 1.39527 train_acc= 0.57205 val_loss= 0.65834 val_acc= 0.69437 test_loss= 0.65035 test_acc= 0.68571 time= 0.17846\n",
            "INFO:training_log:\n",
            " Epoch: 0106 train_loss= 1.39527 train_acc= 0.57205 val_loss= 0.65834 val_acc= 0.69437 test_loss= 0.65035 test_acc= 0.68571 time= 0.17846\n",
            "2024-04-20 10:54:35,034 - INFO - \n",
            " Epoch: 0107 train_loss= 1.38657 train_acc= 0.57596 val_loss= 0.65675 val_acc= 0.69437 test_loss= 0.64901 test_acc= 0.68683 time= 0.17742\n",
            "INFO:training_log:\n",
            " Epoch: 0107 train_loss= 1.38657 train_acc= 0.57596 val_loss= 0.65675 val_acc= 0.69437 test_loss= 0.64901 test_acc= 0.68683 time= 0.17742\n",
            "2024-04-20 10:54:35,218 - INFO - \n",
            " Epoch: 0108 train_loss= 1.37373 train_acc= 0.56486 val_loss= 0.65771 val_acc= 0.69577 test_loss= 0.65013 test_acc= 0.68571 time= 0.17972\n",
            "INFO:training_log:\n",
            " Epoch: 0108 train_loss= 1.37373 train_acc= 0.56486 val_loss= 0.65771 val_acc= 0.69577 test_loss= 0.65013 test_acc= 0.68571 time= 0.17972\n",
            "2024-04-20 10:54:35,401 - INFO - \n",
            " Epoch: 0109 train_loss= 1.36363 train_acc= 0.57909 val_loss= 0.65268 val_acc= 0.70423 test_loss= 0.64545 test_acc= 0.68936 time= 0.17789\n",
            "INFO:training_log:\n",
            " Epoch: 0109 train_loss= 1.36363 train_acc= 0.57909 val_loss= 0.65268 val_acc= 0.70423 test_loss= 0.64545 test_acc= 0.68936 time= 0.17789\n",
            "2024-04-20 10:54:35,584 - INFO - \n",
            " Epoch: 0110 train_loss= 1.32996 train_acc= 0.58393 val_loss= 0.64049 val_acc= 0.71408 test_loss= 0.63386 test_acc= 0.69387 time= 0.17768\n",
            "INFO:training_log:\n",
            " Epoch: 0110 train_loss= 1.32996 train_acc= 0.58393 val_loss= 0.64049 val_acc= 0.71408 test_loss= 0.63386 test_acc= 0.69387 time= 0.17768\n",
            "2024-04-20 10:54:35,767 - INFO - \n",
            " Epoch: 0111 train_loss= 1.37243 train_acc= 0.56330 val_loss= 0.62972 val_acc= 0.71408 test_loss= 0.62371 test_acc= 0.69499 time= 0.17899\n",
            "INFO:training_log:\n",
            " Epoch: 0111 train_loss= 1.37243 train_acc= 0.56330 val_loss= 0.62972 val_acc= 0.71408 test_loss= 0.62371 test_acc= 0.69499 time= 0.17899\n",
            "2024-04-20 10:54:35,948 - INFO - \n",
            " Epoch: 0112 train_loss= 1.27140 train_acc= 0.59315 val_loss= 0.62200 val_acc= 0.71268 test_loss= 0.61647 test_acc= 0.69977 time= 0.17703\n",
            "INFO:training_log:\n",
            " Epoch: 0112 train_loss= 1.27140 train_acc= 0.59315 val_loss= 0.62200 val_acc= 0.71268 test_loss= 0.61647 test_acc= 0.69977 time= 0.17703\n",
            "2024-04-20 10:54:36,131 - INFO - \n",
            " Epoch: 0113 train_loss= 1.29321 train_acc= 0.59847 val_loss= 0.61879 val_acc= 0.70986 test_loss= 0.61350 test_acc= 0.70062 time= 0.17780\n",
            "INFO:training_log:\n",
            " Epoch: 0113 train_loss= 1.29321 train_acc= 0.59847 val_loss= 0.61879 val_acc= 0.70986 test_loss= 0.61350 test_acc= 0.70062 time= 0.17780\n",
            "2024-04-20 10:54:36,314 - INFO - \n",
            " Epoch: 0114 train_loss= 1.36531 train_acc= 0.57065 val_loss= 0.61844 val_acc= 0.71268 test_loss= 0.61319 test_acc= 0.70034 time= 0.17906\n",
            "INFO:training_log:\n",
            " Epoch: 0114 train_loss= 1.36531 train_acc= 0.57065 val_loss= 0.61844 val_acc= 0.71268 test_loss= 0.61319 test_acc= 0.70034 time= 0.17906\n",
            "2024-04-20 10:54:36,498 - INFO - \n",
            " Epoch: 0115 train_loss= 1.29814 train_acc= 0.57268 val_loss= 0.61535 val_acc= 0.70845 test_loss= 0.61034 test_acc= 0.70090 time= 0.17874\n",
            "INFO:training_log:\n",
            " Epoch: 0115 train_loss= 1.29814 train_acc= 0.57268 val_loss= 0.61535 val_acc= 0.70845 test_loss= 0.61034 test_acc= 0.70090 time= 0.17874\n",
            "2024-04-20 10:54:36,681 - INFO - \n",
            " Epoch: 0116 train_loss= 1.36615 train_acc= 0.57127 val_loss= 0.61371 val_acc= 0.70704 test_loss= 0.60884 test_acc= 0.70090 time= 0.17811\n",
            "INFO:training_log:\n",
            " Epoch: 0116 train_loss= 1.36615 train_acc= 0.57127 val_loss= 0.61371 val_acc= 0.70704 test_loss= 0.60884 test_acc= 0.70090 time= 0.17811\n",
            "2024-04-20 10:54:36,864 - INFO - \n",
            " Epoch: 0117 train_loss= 1.25598 train_acc= 0.59331 val_loss= 0.61043 val_acc= 0.70986 test_loss= 0.60579 test_acc= 0.70259 time= 0.17774\n",
            "INFO:training_log:\n",
            " Epoch: 0117 train_loss= 1.25598 train_acc= 0.59331 val_loss= 0.61043 val_acc= 0.70986 test_loss= 0.60579 test_acc= 0.70259 time= 0.17774\n",
            "2024-04-20 10:54:37,046 - INFO - \n",
            " Epoch: 0118 train_loss= 1.21144 train_acc= 0.61566 val_loss= 0.60908 val_acc= 0.70704 test_loss= 0.60456 test_acc= 0.70371 time= 0.17754\n",
            "INFO:training_log:\n",
            " Epoch: 0118 train_loss= 1.21144 train_acc= 0.61566 val_loss= 0.60908 val_acc= 0.70704 test_loss= 0.60456 test_acc= 0.70371 time= 0.17754\n",
            "2024-04-20 10:54:37,227 - INFO - \n",
            " Epoch: 0119 train_loss= 1.24399 train_acc= 0.59050 val_loss= 0.60710 val_acc= 0.70986 test_loss= 0.60274 test_acc= 0.70371 time= 0.17727\n",
            "INFO:training_log:\n",
            " Epoch: 0119 train_loss= 1.24399 train_acc= 0.59050 val_loss= 0.60710 val_acc= 0.70986 test_loss= 0.60274 test_acc= 0.70371 time= 0.17727\n",
            "2024-04-20 10:54:37,410 - INFO - \n",
            " Epoch: 0120 train_loss= 1.27491 train_acc= 0.59956 val_loss= 0.60508 val_acc= 0.70563 test_loss= 0.60093 test_acc= 0.70428 time= 0.17812\n",
            "INFO:training_log:\n",
            " Epoch: 0120 train_loss= 1.27491 train_acc= 0.59956 val_loss= 0.60508 val_acc= 0.70563 test_loss= 0.60093 test_acc= 0.70428 time= 0.17812\n",
            "2024-04-20 10:54:37,422 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:37,427 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:38,063 - INFO - Current best loss 0.60508\n",
            "INFO:training_log:Current best loss 0.60508\n",
            "2024-04-20 10:54:38,247 - INFO - \n",
            " Epoch: 0121 train_loss= 1.66022 train_acc= 0.56440 val_loss= 0.61316 val_acc= 0.71408 test_loss= 0.60855 test_acc= 0.70400 time= 0.17799\n",
            "INFO:training_log:\n",
            " Epoch: 0121 train_loss= 1.66022 train_acc= 0.56440 val_loss= 0.61316 val_acc= 0.71408 test_loss= 0.60855 test_acc= 0.70400 time= 0.17799\n",
            "2024-04-20 10:54:38,430 - INFO - \n",
            " Epoch: 0122 train_loss= 1.57956 train_acc= 0.59612 val_loss= 0.60989 val_acc= 0.71408 test_loss= 0.60559 test_acc= 0.70540 time= 0.17885\n",
            "INFO:training_log:\n",
            " Epoch: 0122 train_loss= 1.57956 train_acc= 0.59612 val_loss= 0.60989 val_acc= 0.71408 test_loss= 0.60559 test_acc= 0.70540 time= 0.17885\n",
            "2024-04-20 10:54:38,619 - INFO - \n",
            " Epoch: 0123 train_loss= 1.43396 train_acc= 0.57362 val_loss= 0.60076 val_acc= 0.70423 test_loss= 0.59725 test_acc= 0.70737 time= 0.18433\n",
            "INFO:training_log:\n",
            " Epoch: 0123 train_loss= 1.43396 train_acc= 0.57362 val_loss= 0.60076 val_acc= 0.70423 test_loss= 0.59725 test_acc= 0.70737 time= 0.18433\n",
            "2024-04-20 10:54:38,631 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:38,636 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:39,364 - INFO - Current best loss 0.60076\n",
            "INFO:training_log:Current best loss 0.60076\n",
            "2024-04-20 10:54:39,551 - INFO - \n",
            " Epoch: 0124 train_loss= 1.26230 train_acc= 0.59878 val_loss= 0.59741 val_acc= 0.70563 test_loss= 0.59424 test_acc= 0.70765 time= 0.18092\n",
            "INFO:training_log:\n",
            " Epoch: 0124 train_loss= 1.26230 train_acc= 0.59878 val_loss= 0.59741 val_acc= 0.70563 test_loss= 0.59424 test_acc= 0.70765 time= 0.18092\n",
            "2024-04-20 10:54:39,564 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:39,569 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:40,257 - INFO - Current best loss 0.59741\n",
            "INFO:training_log:Current best loss 0.59741\n",
            "2024-04-20 10:54:40,442 - INFO - \n",
            " Epoch: 0125 train_loss= 1.40169 train_acc= 0.58315 val_loss= 0.59128 val_acc= 0.71127 test_loss= 0.58879 test_acc= 0.71131 time= 0.17730\n",
            "INFO:training_log:\n",
            " Epoch: 0125 train_loss= 1.40169 train_acc= 0.58315 val_loss= 0.59128 val_acc= 0.71127 test_loss= 0.58879 test_acc= 0.71131 time= 0.17730\n",
            "2024-04-20 10:54:40,453 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:40,457 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:42,928 - INFO - Current best loss 0.59128\n",
            "INFO:training_log:Current best loss 0.59128\n",
            "2024-04-20 10:54:43,194 - INFO - \n",
            " Epoch: 0126 train_loss= 1.34643 train_acc= 0.56486 val_loss= 0.58958 val_acc= 0.70845 test_loss= 0.58729 test_acc= 0.71075 time= 0.25387\n",
            "INFO:training_log:\n",
            " Epoch: 0126 train_loss= 1.34643 train_acc= 0.56486 val_loss= 0.58958 val_acc= 0.70845 test_loss= 0.58729 test_acc= 0.71075 time= 0.25387\n",
            "2024-04-20 10:54:43,210 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:43,215 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:44,337 - INFO - Current best loss 0.58958\n",
            "INFO:training_log:Current best loss 0.58958\n",
            "2024-04-20 10:54:44,527 - INFO - \n",
            " Epoch: 0127 train_loss= 1.28756 train_acc= 0.59550 val_loss= 0.58873 val_acc= 0.70704 test_loss= 0.58652 test_acc= 0.70990 time= 0.18592\n",
            "INFO:training_log:\n",
            " Epoch: 0127 train_loss= 1.28756 train_acc= 0.59550 val_loss= 0.58873 val_acc= 0.70704 test_loss= 0.58652 test_acc= 0.70990 time= 0.18592\n",
            "2024-04-20 10:54:44,547 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:44,552 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:45,234 - INFO - Current best loss 0.58873\n",
            "INFO:training_log:Current best loss 0.58873\n",
            "2024-04-20 10:54:45,415 - INFO - \n",
            " Epoch: 0128 train_loss= 1.32535 train_acc= 0.61472 val_loss= 0.58586 val_acc= 0.70141 test_loss= 0.58425 test_acc= 0.71131 time= 0.17630\n",
            "INFO:training_log:\n",
            " Epoch: 0128 train_loss= 1.32535 train_acc= 0.61472 val_loss= 0.58586 val_acc= 0.70141 test_loss= 0.58425 test_acc= 0.71131 time= 0.17630\n",
            "2024-04-20 10:54:45,427 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:45,430 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:46,844 - INFO - Current best loss 0.58586\n",
            "INFO:training_log:Current best loss 0.58586\n",
            "2024-04-20 10:54:47,025 - INFO - \n",
            " Epoch: 0129 train_loss= 1.24428 train_acc= 0.59659 val_loss= 0.58461 val_acc= 0.70141 test_loss= 0.58383 test_acc= 0.70259 time= 0.17651\n",
            "INFO:training_log:\n",
            " Epoch: 0129 train_loss= 1.24428 train_acc= 0.59659 val_loss= 0.58461 val_acc= 0.70141 test_loss= 0.58383 test_acc= 0.70259 time= 0.17651\n",
            "2024-04-20 10:54:47,039 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:47,042 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:48,939 - INFO - Current best loss 0.58461\n",
            "INFO:training_log:Current best loss 0.58461\n",
            "2024-04-20 10:54:49,195 - INFO - \n",
            " Epoch: 0130 train_loss= 1.38852 train_acc= 0.58034 val_loss= 0.58455 val_acc= 0.69859 test_loss= 0.58412 test_acc= 0.70259 time= 0.25182\n",
            "INFO:training_log:\n",
            " Epoch: 0130 train_loss= 1.38852 train_acc= 0.58034 val_loss= 0.58455 val_acc= 0.69859 test_loss= 0.58412 test_acc= 0.70259 time= 0.25182\n",
            "2024-04-20 10:54:49,209 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:49,215 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:53,002 - INFO - Current best loss 0.58455\n",
            "INFO:training_log:Current best loss 0.58455\n",
            "2024-04-20 10:54:53,252 - INFO - \n",
            " Epoch: 0131 train_loss= 1.30297 train_acc= 0.59269 val_loss= 0.58446 val_acc= 0.69437 test_loss= 0.58426 test_acc= 0.70203 time= 0.24455\n",
            "INFO:training_log:\n",
            " Epoch: 0131 train_loss= 1.30297 train_acc= 0.59269 val_loss= 0.58446 val_acc= 0.69437 test_loss= 0.58426 test_acc= 0.70203 time= 0.24455\n",
            "2024-04-20 10:54:53,686 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:53,696 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:54,720 - INFO - Current best loss 0.58446\n",
            "INFO:training_log:Current best loss 0.58446\n",
            "2024-04-20 10:54:54,927 - INFO - \n",
            " Epoch: 0132 train_loss= 1.32335 train_acc= 0.58018 val_loss= 0.58373 val_acc= 0.69859 test_loss= 0.58334 test_acc= 0.70343 time= 0.20254\n",
            "INFO:training_log:\n",
            " Epoch: 0132 train_loss= 1.32335 train_acc= 0.58018 val_loss= 0.58373 val_acc= 0.69859 test_loss= 0.58334 test_acc= 0.70343 time= 0.20254\n",
            "2024-04-20 10:54:54,941 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:54,949 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:57,995 - INFO - Current best loss 0.58373\n",
            "INFO:training_log:Current best loss 0.58373\n",
            "2024-04-20 10:54:58,251 - INFO - \n",
            " Epoch: 0133 train_loss= 1.43188 train_acc= 0.58800 val_loss= 0.58337 val_acc= 0.69859 test_loss= 0.58224 test_acc= 0.70934 time= 0.25001\n",
            "INFO:training_log:\n",
            " Epoch: 0133 train_loss= 1.43188 train_acc= 0.58800 val_loss= 0.58337 val_acc= 0.69859 test_loss= 0.58224 test_acc= 0.70934 time= 0.25001\n",
            "2024-04-20 10:54:58,730 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:54:58,734 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:54:59,459 - INFO - Current best loss 0.58337\n",
            "INFO:training_log:Current best loss 0.58337\n",
            "2024-04-20 10:54:59,650 - INFO - \n",
            " Epoch: 0134 train_loss= 1.24037 train_acc= 0.60144 val_loss= 0.58400 val_acc= 0.70282 test_loss= 0.58243 test_acc= 0.71159 time= 0.18702\n",
            "INFO:training_log:\n",
            " Epoch: 0134 train_loss= 1.24037 train_acc= 0.60144 val_loss= 0.58400 val_acc= 0.70282 test_loss= 0.58243 test_acc= 0.71159 time= 0.18702\n",
            "2024-04-20 10:54:59,836 - INFO - \n",
            " Epoch: 0135 train_loss= 1.43111 train_acc= 0.57643 val_loss= 0.58783 val_acc= 0.71408 test_loss= 0.58539 test_acc= 0.71187 time= 0.17942\n",
            "INFO:training_log:\n",
            " Epoch: 0135 train_loss= 1.43111 train_acc= 0.57643 val_loss= 0.58783 val_acc= 0.71408 test_loss= 0.58539 test_acc= 0.71187 time= 0.17942\n",
            "2024-04-20 10:55:00,019 - INFO - \n",
            " Epoch: 0136 train_loss= 1.28868 train_acc= 0.61285 val_loss= 0.59377 val_acc= 0.71408 test_loss= 0.59056 test_acc= 0.71412 time= 0.17768\n",
            "INFO:training_log:\n",
            " Epoch: 0136 train_loss= 1.28868 train_acc= 0.61285 val_loss= 0.59377 val_acc= 0.71408 test_loss= 0.59056 test_acc= 0.71412 time= 0.17768\n",
            "2024-04-20 10:55:00,202 - INFO - \n",
            " Epoch: 0137 train_loss= 1.28808 train_acc= 0.59894 val_loss= 0.59816 val_acc= 0.71268 test_loss= 0.59448 test_acc= 0.71328 time= 0.17855\n",
            "INFO:training_log:\n",
            " Epoch: 0137 train_loss= 1.28808 train_acc= 0.59894 val_loss= 0.59816 val_acc= 0.71268 test_loss= 0.59448 test_acc= 0.71328 time= 0.17855\n",
            "2024-04-20 10:55:00,389 - INFO - \n",
            " Epoch: 0138 train_loss= 1.20252 train_acc= 0.61316 val_loss= 0.60718 val_acc= 0.71690 test_loss= 0.60284 test_acc= 0.71075 time= 0.17787\n",
            "INFO:training_log:\n",
            " Epoch: 0138 train_loss= 1.20252 train_acc= 0.61316 val_loss= 0.60718 val_acc= 0.71690 test_loss= 0.60284 test_acc= 0.71075 time= 0.17787\n",
            "2024-04-20 10:55:00,572 - INFO - \n",
            " Epoch: 0139 train_loss= 1.19678 train_acc= 0.60957 val_loss= 0.61631 val_acc= 0.72394 test_loss= 0.61147 test_acc= 0.70625 time= 0.17731\n",
            "INFO:training_log:\n",
            " Epoch: 0139 train_loss= 1.19678 train_acc= 0.60957 val_loss= 0.61631 val_acc= 0.72394 test_loss= 0.61147 test_acc= 0.70625 time= 0.17731\n",
            "2024-04-20 10:55:00,756 - INFO - \n",
            " Epoch: 0140 train_loss= 1.22655 train_acc= 0.60269 val_loss= 0.62561 val_acc= 0.72394 test_loss= 0.62038 test_acc= 0.70287 time= 0.17809\n",
            "INFO:training_log:\n",
            " Epoch: 0140 train_loss= 1.22655 train_acc= 0.60269 val_loss= 0.62561 val_acc= 0.72394 test_loss= 0.62038 test_acc= 0.70287 time= 0.17809\n",
            "2024-04-20 10:55:00,939 - INFO - \n",
            " Epoch: 0141 train_loss= 1.21520 train_acc= 0.61254 val_loss= 0.63558 val_acc= 0.72535 test_loss= 0.62998 test_acc= 0.70090 time= 0.17903\n",
            "INFO:training_log:\n",
            " Epoch: 0141 train_loss= 1.21520 train_acc= 0.61254 val_loss= 0.63558 val_acc= 0.72535 test_loss= 0.62998 test_acc= 0.70090 time= 0.17903\n",
            "2024-04-20 10:55:01,122 - INFO - \n",
            " Epoch: 0142 train_loss= 1.42296 train_acc= 0.60066 val_loss= 0.63832 val_acc= 0.72817 test_loss= 0.63264 test_acc= 0.70034 time= 0.17863\n",
            "INFO:training_log:\n",
            " Epoch: 0142 train_loss= 1.42296 train_acc= 0.60066 val_loss= 0.63832 val_acc= 0.72817 test_loss= 0.63264 test_acc= 0.70034 time= 0.17863\n",
            "2024-04-20 10:55:01,305 - INFO - \n",
            " Epoch: 0143 train_loss= 1.23315 train_acc= 0.59800 val_loss= 0.63717 val_acc= 0.72676 test_loss= 0.63152 test_acc= 0.70062 time= 0.17882\n",
            "INFO:training_log:\n",
            " Epoch: 0143 train_loss= 1.23315 train_acc= 0.59800 val_loss= 0.63717 val_acc= 0.72676 test_loss= 0.63152 test_acc= 0.70062 time= 0.17882\n",
            "2024-04-20 10:55:01,488 - INFO - \n",
            " Epoch: 0144 train_loss= 1.40100 train_acc= 0.59534 val_loss= 0.62556 val_acc= 0.72676 test_loss= 0.62031 test_acc= 0.70400 time= 0.17900\n",
            "INFO:training_log:\n",
            " Epoch: 0144 train_loss= 1.40100 train_acc= 0.59534 val_loss= 0.62556 val_acc= 0.72676 test_loss= 0.62031 test_acc= 0.70400 time= 0.17900\n",
            "2024-04-20 10:55:01,671 - INFO - \n",
            " Epoch: 0145 train_loss= 1.23574 train_acc= 0.60128 val_loss= 0.62029 val_acc= 0.72394 test_loss= 0.61528 test_acc= 0.70371 time= 0.17882\n",
            "INFO:training_log:\n",
            " Epoch: 0145 train_loss= 1.23574 train_acc= 0.60128 val_loss= 0.62029 val_acc= 0.72394 test_loss= 0.61528 test_acc= 0.70371 time= 0.17882\n",
            "2024-04-20 10:55:01,853 - INFO - \n",
            " Epoch: 0146 train_loss= 1.33253 train_acc= 0.59737 val_loss= 0.61779 val_acc= 0.72254 test_loss= 0.61293 test_acc= 0.70568 time= 0.17881\n",
            "INFO:training_log:\n",
            " Epoch: 0146 train_loss= 1.33253 train_acc= 0.59737 val_loss= 0.61779 val_acc= 0.72254 test_loss= 0.61293 test_acc= 0.70568 time= 0.17881\n",
            "2024-04-20 10:55:02,040 - INFO - \n",
            " Epoch: 0147 train_loss= 1.42310 train_acc= 0.60738 val_loss= 0.60593 val_acc= 0.72254 test_loss= 0.60165 test_acc= 0.71216 time= 0.18185\n",
            "INFO:training_log:\n",
            " Epoch: 0147 train_loss= 1.42310 train_acc= 0.60738 val_loss= 0.60593 val_acc= 0.72254 test_loss= 0.60165 test_acc= 0.71216 time= 0.18185\n",
            "2024-04-20 10:55:02,224 - INFO - \n",
            " Epoch: 0148 train_loss= 1.45527 train_acc= 0.58284 val_loss= 0.59028 val_acc= 0.71972 test_loss= 0.58710 test_acc= 0.71497 time= 0.17923\n",
            "INFO:training_log:\n",
            " Epoch: 0148 train_loss= 1.45527 train_acc= 0.58284 val_loss= 0.59028 val_acc= 0.71972 test_loss= 0.58710 test_acc= 0.71497 time= 0.17923\n",
            "2024-04-20 10:55:02,409 - INFO - \n",
            " Epoch: 0149 train_loss= 1.27426 train_acc= 0.60581 val_loss= 0.58224 val_acc= 0.71268 test_loss= 0.58008 test_acc= 0.71272 time= 0.18026\n",
            "INFO:training_log:\n",
            " Epoch: 0149 train_loss= 1.27426 train_acc= 0.60581 val_loss= 0.58224 val_acc= 0.71268 test_loss= 0.58008 test_acc= 0.71272 time= 0.18026\n",
            "2024-04-20 10:55:02,422 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:55:02,427 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:55:03,221 - INFO - Current best loss 0.58224\n",
            "INFO:training_log:Current best loss 0.58224\n",
            "2024-04-20 10:55:03,416 - INFO - \n",
            " Epoch: 0150 train_loss= 1.27213 train_acc= 0.60409 val_loss= 0.57901 val_acc= 0.70986 test_loss= 0.57812 test_acc= 0.71075 time= 0.18665\n",
            "INFO:training_log:\n",
            " Epoch: 0150 train_loss= 1.27213 train_acc= 0.60409 val_loss= 0.57901 val_acc= 0.70986 test_loss= 0.57812 test_acc= 0.71075 time= 0.18665\n",
            "2024-04-20 10:55:03,429 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:55:03,433 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:55:04,123 - INFO - Current best loss 0.57901\n",
            "INFO:training_log:Current best loss 0.57901\n",
            "2024-04-20 10:55:04,309 - INFO - \n",
            " Epoch: 0151 train_loss= 1.26786 train_acc= 0.59112 val_loss= 0.58213 val_acc= 0.69577 test_loss= 0.58247 test_acc= 0.70850 time= 0.17923\n",
            "INFO:training_log:\n",
            " Epoch: 0151 train_loss= 1.26786 train_acc= 0.59112 val_loss= 0.58213 val_acc= 0.69577 test_loss= 0.58247 test_acc= 0.70850 time= 0.17923\n",
            "2024-04-20 10:55:04,495 - INFO - \n",
            " Epoch: 0152 train_loss= 1.26370 train_acc= 0.59112 val_loss= 0.58715 val_acc= 0.68873 test_loss= 0.58828 test_acc= 0.70540 time= 0.18035\n",
            "INFO:training_log:\n",
            " Epoch: 0152 train_loss= 1.26370 train_acc= 0.59112 val_loss= 0.58715 val_acc= 0.68873 test_loss= 0.58828 test_acc= 0.70540 time= 0.18035\n",
            "2024-04-20 10:55:04,682 - INFO - \n",
            " Epoch: 0153 train_loss= 1.24234 train_acc= 0.61394 val_loss= 0.59728 val_acc= 0.69014 test_loss= 0.59931 test_acc= 0.70174 time= 0.17831\n",
            "INFO:training_log:\n",
            " Epoch: 0153 train_loss= 1.24234 train_acc= 0.61394 val_loss= 0.59728 val_acc= 0.69014 test_loss= 0.59931 test_acc= 0.70174 time= 0.17831\n",
            "2024-04-20 10:55:04,865 - INFO - \n",
            " Epoch: 0154 train_loss= 1.37112 train_acc= 0.58675 val_loss= 0.60496 val_acc= 0.68873 test_loss= 0.60738 test_acc= 0.69471 time= 0.17749\n",
            "INFO:training_log:\n",
            " Epoch: 0154 train_loss= 1.37112 train_acc= 0.58675 val_loss= 0.60496 val_acc= 0.68873 test_loss= 0.60738 test_acc= 0.69471 time= 0.17749\n",
            "2024-04-20 10:55:05,053 - INFO - \n",
            " Epoch: 0155 train_loss= 1.46815 train_acc= 0.58081 val_loss= 0.60240 val_acc= 0.69014 test_loss= 0.60461 test_acc= 0.69809 time= 0.17913\n",
            "INFO:training_log:\n",
            " Epoch: 0155 train_loss= 1.46815 train_acc= 0.58081 val_loss= 0.60240 val_acc= 0.69014 test_loss= 0.60461 test_acc= 0.69809 time= 0.17913\n",
            "2024-04-20 10:55:05,239 - INFO - \n",
            " Epoch: 0156 train_loss= 1.38570 train_acc= 0.59675 val_loss= 0.59343 val_acc= 0.69014 test_loss= 0.59497 test_acc= 0.70343 time= 0.18233\n",
            "INFO:training_log:\n",
            " Epoch: 0156 train_loss= 1.38570 train_acc= 0.59675 val_loss= 0.59343 val_acc= 0.69014 test_loss= 0.59497 test_acc= 0.70343 time= 0.18233\n",
            "2024-04-20 10:55:05,422 - INFO - \n",
            " Epoch: 0157 train_loss= 1.24008 train_acc= 0.59972 val_loss= 0.58419 val_acc= 0.69296 test_loss= 0.58484 test_acc= 0.70990 time= 0.17701\n",
            "INFO:training_log:\n",
            " Epoch: 0157 train_loss= 1.24008 train_acc= 0.59972 val_loss= 0.58419 val_acc= 0.69296 test_loss= 0.58484 test_acc= 0.70990 time= 0.17701\n",
            "2024-04-20 10:55:05,605 - INFO - \n",
            " Epoch: 0158 train_loss= 1.63758 train_acc= 0.57987 val_loss= 0.57705 val_acc= 0.71127 test_loss= 0.57614 test_acc= 0.71553 time= 0.17916\n",
            "INFO:training_log:\n",
            " Epoch: 0158 train_loss= 1.63758 train_acc= 0.57987 val_loss= 0.57705 val_acc= 0.71127 test_loss= 0.57614 test_acc= 0.71553 time= 0.17916\n",
            "2024-04-20 10:55:05,618 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:55:05,623 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:55:08,024 - INFO - Current best loss 0.57705\n",
            "INFO:training_log:Current best loss 0.57705\n",
            "2024-04-20 10:55:08,478 - INFO - \n",
            " Epoch: 0159 train_loss= 1.27386 train_acc= 0.61160 val_loss= 0.58207 val_acc= 0.71690 test_loss= 0.57943 test_acc= 0.71806 time= 0.44097\n",
            "INFO:training_log:\n",
            " Epoch: 0159 train_loss= 1.27386 train_acc= 0.61160 val_loss= 0.58207 val_acc= 0.71690 test_loss= 0.57943 test_acc= 0.71806 time= 0.44097\n",
            "2024-04-20 10:55:08,748 - INFO - \n",
            " Epoch: 0160 train_loss= 1.26443 train_acc= 0.61644 val_loss= 0.59355 val_acc= 0.72535 test_loss= 0.58976 test_acc= 0.72116 time= 0.25426\n",
            "INFO:training_log:\n",
            " Epoch: 0160 train_loss= 1.26443 train_acc= 0.61644 val_loss= 0.59355 val_acc= 0.72535 test_loss= 0.58976 test_acc= 0.72116 time= 0.25426\n",
            "2024-04-20 10:55:08,929 - INFO - \n",
            " Epoch: 0161 train_loss= 1.30992 train_acc= 0.60581 val_loss= 0.60612 val_acc= 0.72113 test_loss= 0.60156 test_acc= 0.71300 time= 0.17697\n",
            "INFO:training_log:\n",
            " Epoch: 0161 train_loss= 1.30992 train_acc= 0.60581 val_loss= 0.60612 val_acc= 0.72113 test_loss= 0.60156 test_acc= 0.71300 time= 0.17697\n",
            "2024-04-20 10:55:09,112 - INFO - \n",
            " Epoch: 0162 train_loss= 1.27728 train_acc= 0.60269 val_loss= 0.61571 val_acc= 0.73099 test_loss= 0.61067 test_acc= 0.71047 time= 0.17861\n",
            "INFO:training_log:\n",
            " Epoch: 0162 train_loss= 1.27728 train_acc= 0.60269 val_loss= 0.61571 val_acc= 0.73099 test_loss= 0.61067 test_acc= 0.71047 time= 0.17861\n",
            "2024-04-20 10:55:09,295 - INFO - \n",
            " Epoch: 0163 train_loss= 1.30624 train_acc= 0.58284 val_loss= 0.61724 val_acc= 0.73099 test_loss= 0.61210 test_acc= 0.70822 time= 0.17888\n",
            "INFO:training_log:\n",
            " Epoch: 0163 train_loss= 1.30624 train_acc= 0.58284 val_loss= 0.61724 val_acc= 0.73099 test_loss= 0.61210 test_acc= 0.70822 time= 0.17888\n",
            "2024-04-20 10:55:09,480 - INFO - \n",
            " Epoch: 0164 train_loss= 1.20648 train_acc= 0.63192 val_loss= 0.62394 val_acc= 0.72817 test_loss= 0.61847 test_acc= 0.70512 time= 0.17824\n",
            "INFO:training_log:\n",
            " Epoch: 0164 train_loss= 1.20648 train_acc= 0.63192 val_loss= 0.62394 val_acc= 0.72817 test_loss= 0.61847 test_acc= 0.70512 time= 0.17824\n",
            "2024-04-20 10:55:09,665 - INFO - \n",
            " Epoch: 0165 train_loss= 1.11581 train_acc= 0.63129 val_loss= 0.63256 val_acc= 0.71972 test_loss= 0.62674 test_acc= 0.70400 time= 0.18074\n",
            "INFO:training_log:\n",
            " Epoch: 0165 train_loss= 1.11581 train_acc= 0.63129 val_loss= 0.63256 val_acc= 0.71972 test_loss= 0.62674 test_acc= 0.70400 time= 0.18074\n",
            "2024-04-20 10:55:09,850 - INFO - \n",
            " Epoch: 0166 train_loss= 1.34609 train_acc= 0.58675 val_loss= 0.62980 val_acc= 0.72113 test_loss= 0.62399 test_acc= 0.70315 time= 0.17987\n",
            "INFO:training_log:\n",
            " Epoch: 0166 train_loss= 1.34609 train_acc= 0.58675 val_loss= 0.62980 val_acc= 0.72113 test_loss= 0.62399 test_acc= 0.70315 time= 0.17987\n",
            "2024-04-20 10:55:10,034 - INFO - \n",
            " Epoch: 0167 train_loss= 1.28894 train_acc= 0.61676 val_loss= 0.61691 val_acc= 0.73239 test_loss= 0.61152 test_acc= 0.70878 time= 0.17911\n",
            "INFO:training_log:\n",
            " Epoch: 0167 train_loss= 1.28894 train_acc= 0.61676 val_loss= 0.61691 val_acc= 0.73239 test_loss= 0.61152 test_acc= 0.70878 time= 0.17911\n",
            "2024-04-20 10:55:10,218 - INFO - \n",
            " Epoch: 0168 train_loss= 1.17130 train_acc= 0.59894 val_loss= 0.60843 val_acc= 0.72535 test_loss= 0.60337 test_acc= 0.71244 time= 0.17921\n",
            "INFO:training_log:\n",
            " Epoch: 0168 train_loss= 1.17130 train_acc= 0.59894 val_loss= 0.60843 val_acc= 0.72535 test_loss= 0.60337 test_acc= 0.71244 time= 0.17921\n",
            "2024-04-20 10:55:10,402 - INFO - \n",
            " Epoch: 0169 train_loss= 1.21439 train_acc= 0.63301 val_loss= 0.60603 val_acc= 0.72394 test_loss= 0.60099 test_acc= 0.71328 time= 0.17870\n",
            "INFO:training_log:\n",
            " Epoch: 0169 train_loss= 1.21439 train_acc= 0.63301 val_loss= 0.60603 val_acc= 0.72394 test_loss= 0.60099 test_acc= 0.71328 time= 0.17870\n",
            "2024-04-20 10:55:10,587 - INFO - \n",
            " Epoch: 0170 train_loss= 1.28972 train_acc= 0.59831 val_loss= 0.59702 val_acc= 0.72535 test_loss= 0.59241 test_acc= 0.71694 time= 0.18104\n",
            "INFO:training_log:\n",
            " Epoch: 0170 train_loss= 1.28972 train_acc= 0.59831 val_loss= 0.59702 val_acc= 0.72535 test_loss= 0.59241 test_acc= 0.71694 time= 0.18104\n",
            "2024-04-20 10:55:10,772 - INFO - \n",
            " Epoch: 0171 train_loss= 1.21676 train_acc= 0.59018 val_loss= 0.59319 val_acc= 0.72535 test_loss= 0.58880 test_acc= 0.71975 time= 0.18042\n",
            "INFO:training_log:\n",
            " Epoch: 0171 train_loss= 1.21676 train_acc= 0.59018 val_loss= 0.59319 val_acc= 0.72535 test_loss= 0.58880 test_acc= 0.71975 time= 0.18042\n",
            "2024-04-20 10:55:10,958 - INFO - \n",
            " Epoch: 0172 train_loss= 1.17368 train_acc= 0.62051 val_loss= 0.59030 val_acc= 0.72254 test_loss= 0.58606 test_acc= 0.72088 time= 0.18105\n",
            "INFO:training_log:\n",
            " Epoch: 0172 train_loss= 1.17368 train_acc= 0.62051 val_loss= 0.59030 val_acc= 0.72254 test_loss= 0.58606 test_acc= 0.72088 time= 0.18105\n",
            "2024-04-20 10:55:11,142 - INFO - \n",
            " Epoch: 0173 train_loss= 1.17063 train_acc= 0.61269 val_loss= 0.59191 val_acc= 0.72535 test_loss= 0.58754 test_acc= 0.71975 time= 0.17995\n",
            "INFO:training_log:\n",
            " Epoch: 0173 train_loss= 1.17063 train_acc= 0.61269 val_loss= 0.59191 val_acc= 0.72535 test_loss= 0.58754 test_acc= 0.71975 time= 0.17995\n",
            "2024-04-20 10:55:11,325 - INFO - \n",
            " Epoch: 0174 train_loss= 1.19938 train_acc= 0.62348 val_loss= 0.59058 val_acc= 0.72394 test_loss= 0.58626 test_acc= 0.72032 time= 0.17881\n",
            "INFO:training_log:\n",
            " Epoch: 0174 train_loss= 1.19938 train_acc= 0.62348 val_loss= 0.59058 val_acc= 0.72394 test_loss= 0.58626 test_acc= 0.72032 time= 0.17881\n",
            "2024-04-20 10:55:11,507 - INFO - \n",
            " Epoch: 0175 train_loss= 1.19840 train_acc= 0.59909 val_loss= 0.58916 val_acc= 0.72254 test_loss= 0.58491 test_acc= 0.72116 time= 0.17820\n",
            "INFO:training_log:\n",
            " Epoch: 0175 train_loss= 1.19840 train_acc= 0.59909 val_loss= 0.58916 val_acc= 0.72254 test_loss= 0.58491 test_acc= 0.72116 time= 0.17820\n",
            "2024-04-20 10:55:11,691 - INFO - \n",
            " Epoch: 0176 train_loss= 1.27144 train_acc= 0.60738 val_loss= 0.58421 val_acc= 0.71972 test_loss= 0.58032 test_acc= 0.72454 time= 0.17873\n",
            "INFO:training_log:\n",
            " Epoch: 0176 train_loss= 1.27144 train_acc= 0.60738 val_loss= 0.58421 val_acc= 0.71972 test_loss= 0.58032 test_acc= 0.72454 time= 0.17873\n",
            "2024-04-20 10:55:11,874 - INFO - \n",
            " Epoch: 0177 train_loss= 1.28923 train_acc= 0.61410 val_loss= 0.58414 val_acc= 0.71972 test_loss= 0.58019 test_acc= 0.72566 time= 0.17958\n",
            "INFO:training_log:\n",
            " Epoch: 0177 train_loss= 1.28923 train_acc= 0.61410 val_loss= 0.58414 val_acc= 0.71972 test_loss= 0.58019 test_acc= 0.72566 time= 0.17958\n",
            "2024-04-20 10:55:12,057 - INFO - \n",
            " Epoch: 0178 train_loss= 1.29371 train_acc= 0.60347 val_loss= 0.58073 val_acc= 0.71972 test_loss= 0.57703 test_acc= 0.72763 time= 0.17846\n",
            "INFO:training_log:\n",
            " Epoch: 0178 train_loss= 1.29371 train_acc= 0.60347 val_loss= 0.58073 val_acc= 0.71972 test_loss= 0.57703 test_acc= 0.72763 time= 0.17846\n",
            "2024-04-20 10:55:12,240 - INFO - \n",
            " Epoch: 0179 train_loss= 1.47396 train_acc= 0.58675 val_loss= 0.58411 val_acc= 0.71972 test_loss= 0.58010 test_acc= 0.72622 time= 0.17908\n",
            "INFO:training_log:\n",
            " Epoch: 0179 train_loss= 1.47396 train_acc= 0.58675 val_loss= 0.58411 val_acc= 0.71972 test_loss= 0.58010 test_acc= 0.72622 time= 0.17908\n",
            "2024-04-20 10:55:12,429 - INFO - \n",
            " Epoch: 0180 train_loss= 1.27541 train_acc= 0.60394 val_loss= 0.58307 val_acc= 0.71972 test_loss= 0.57914 test_acc= 0.72763 time= 0.17989\n",
            "INFO:training_log:\n",
            " Epoch: 0180 train_loss= 1.27541 train_acc= 0.60394 val_loss= 0.58307 val_acc= 0.71972 test_loss= 0.57914 test_acc= 0.72763 time= 0.17989\n",
            "2024-04-20 10:55:12,616 - INFO - \n",
            " Epoch: 0181 train_loss= 1.19683 train_acc= 0.60660 val_loss= 0.58466 val_acc= 0.72113 test_loss= 0.58060 test_acc= 0.72566 time= 0.18372\n",
            "INFO:training_log:\n",
            " Epoch: 0181 train_loss= 1.19683 train_acc= 0.60660 val_loss= 0.58466 val_acc= 0.72113 test_loss= 0.58060 test_acc= 0.72566 time= 0.18372\n",
            "2024-04-20 10:55:12,802 - INFO - \n",
            " Epoch: 0182 train_loss= 1.27681 train_acc= 0.59378 val_loss= 0.59118 val_acc= 0.72535 test_loss= 0.58666 test_acc= 0.71975 time= 0.18101\n",
            "INFO:training_log:\n",
            " Epoch: 0182 train_loss= 1.27681 train_acc= 0.59378 val_loss= 0.59118 val_acc= 0.72535 test_loss= 0.58666 test_acc= 0.71975 time= 0.18101\n",
            "2024-04-20 10:55:12,988 - INFO - \n",
            " Epoch: 0183 train_loss= 1.26207 train_acc= 0.60144 val_loss= 0.59294 val_acc= 0.72676 test_loss= 0.58825 test_acc= 0.72088 time= 0.18047\n",
            "INFO:training_log:\n",
            " Epoch: 0183 train_loss= 1.26207 train_acc= 0.60144 val_loss= 0.59294 val_acc= 0.72676 test_loss= 0.58825 test_acc= 0.72088 time= 0.18047\n",
            "2024-04-20 10:55:13,173 - INFO - \n",
            " Epoch: 0184 train_loss= 1.11801 train_acc= 0.63207 val_loss= 0.59347 val_acc= 0.72817 test_loss= 0.58868 test_acc= 0.72060 time= 0.17991\n",
            "INFO:training_log:\n",
            " Epoch: 0184 train_loss= 1.11801 train_acc= 0.63207 val_loss= 0.59347 val_acc= 0.72817 test_loss= 0.58868 test_acc= 0.72060 time= 0.17991\n",
            "2024-04-20 10:55:13,360 - INFO - \n",
            " Epoch: 0185 train_loss= 1.13273 train_acc= 0.62113 val_loss= 0.59428 val_acc= 0.72817 test_loss= 0.58937 test_acc= 0.72060 time= 0.17976\n",
            "INFO:training_log:\n",
            " Epoch: 0185 train_loss= 1.13273 train_acc= 0.62113 val_loss= 0.59428 val_acc= 0.72817 test_loss= 0.58937 test_acc= 0.72060 time= 0.17976\n",
            "2024-04-20 10:55:13,543 - INFO - \n",
            " Epoch: 0186 train_loss= 1.18591 train_acc= 0.62004 val_loss= 0.59614 val_acc= 0.73380 test_loss= 0.59110 test_acc= 0.72116 time= 0.17872\n",
            "INFO:training_log:\n",
            " Epoch: 0186 train_loss= 1.18591 train_acc= 0.62004 val_loss= 0.59614 val_acc= 0.73380 test_loss= 0.59110 test_acc= 0.72116 time= 0.17872\n",
            "2024-04-20 10:55:13,728 - INFO - \n",
            " Epoch: 0187 train_loss= 1.16585 train_acc= 0.62098 val_loss= 0.60210 val_acc= 0.73239 test_loss= 0.59667 test_acc= 0.71835 time= 0.18020\n",
            "INFO:training_log:\n",
            " Epoch: 0187 train_loss= 1.16585 train_acc= 0.62098 val_loss= 0.60210 val_acc= 0.73239 test_loss= 0.59667 test_acc= 0.71835 time= 0.18020\n",
            "2024-04-20 10:55:13,914 - INFO - \n",
            " Epoch: 0188 train_loss= 1.23015 train_acc= 0.61816 val_loss= 0.60253 val_acc= 0.73239 test_loss= 0.59696 test_acc= 0.72003 time= 0.18140\n",
            "INFO:training_log:\n",
            " Epoch: 0188 train_loss= 1.23015 train_acc= 0.61816 val_loss= 0.60253 val_acc= 0.73239 test_loss= 0.59696 test_acc= 0.72003 time= 0.18140\n",
            "2024-04-20 10:55:14,099 - INFO - \n",
            " Epoch: 0189 train_loss= 1.14670 train_acc= 0.62238 val_loss= 0.60216 val_acc= 0.73380 test_loss= 0.59652 test_acc= 0.71975 time= 0.18107\n",
            "INFO:training_log:\n",
            " Epoch: 0189 train_loss= 1.14670 train_acc= 0.62238 val_loss= 0.60216 val_acc= 0.73380 test_loss= 0.59652 test_acc= 0.71975 time= 0.18107\n",
            "2024-04-20 10:55:14,286 - INFO - \n",
            " Epoch: 0190 train_loss= 1.22897 train_acc= 0.61316 val_loss= 0.60798 val_acc= 0.72817 test_loss= 0.60195 test_acc= 0.71806 time= 0.18198\n",
            "INFO:training_log:\n",
            " Epoch: 0190 train_loss= 1.22897 train_acc= 0.61316 val_loss= 0.60798 val_acc= 0.72817 test_loss= 0.60195 test_acc= 0.71806 time= 0.18198\n",
            "2024-04-20 10:55:14,469 - INFO - \n",
            " Epoch: 0191 train_loss= 1.15714 train_acc= 0.61707 val_loss= 0.61069 val_acc= 0.72676 test_loss= 0.60442 test_acc= 0.71694 time= 0.17814\n",
            "INFO:training_log:\n",
            " Epoch: 0191 train_loss= 1.15714 train_acc= 0.61707 val_loss= 0.61069 val_acc= 0.72676 test_loss= 0.60442 test_acc= 0.71694 time= 0.17814\n",
            "2024-04-20 10:55:14,655 - INFO - \n",
            " Epoch: 0192 train_loss= 1.14859 train_acc= 0.61941 val_loss= 0.61012 val_acc= 0.72817 test_loss= 0.60377 test_acc= 0.71722 time= 0.18136\n",
            "INFO:training_log:\n",
            " Epoch: 0192 train_loss= 1.14859 train_acc= 0.61941 val_loss= 0.61012 val_acc= 0.72817 test_loss= 0.60377 test_acc= 0.71722 time= 0.18136\n",
            "2024-04-20 10:55:14,838 - INFO - \n",
            " Epoch: 0193 train_loss= 1.20913 train_acc= 0.61676 val_loss= 0.61204 val_acc= 0.73099 test_loss= 0.60554 test_acc= 0.71722 time= 0.17879\n",
            "INFO:training_log:\n",
            " Epoch: 0193 train_loss= 1.20913 train_acc= 0.61676 val_loss= 0.61204 val_acc= 0.73099 test_loss= 0.60554 test_acc= 0.71722 time= 0.17879\n",
            "2024-04-20 10:55:15,022 - INFO - \n",
            " Epoch: 0194 train_loss= 1.07414 train_acc= 0.62441 val_loss= 0.61363 val_acc= 0.73099 test_loss= 0.60699 test_acc= 0.71666 time= 0.18006\n",
            "INFO:training_log:\n",
            " Epoch: 0194 train_loss= 1.07414 train_acc= 0.62441 val_loss= 0.61363 val_acc= 0.73099 test_loss= 0.60699 test_acc= 0.71666 time= 0.18006\n",
            "2024-04-20 10:55:15,025 - INFO - Early stopping...\n",
            "INFO:training_log:Early stopping...\n",
            "2024-04-20 10:55:15,042 - INFO - Configurations for training:\n",
            "INFO:training_log:Configurations for training:\n",
            "2024-04-20 10:55:15,046 - DEBUG - {'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "DEBUG:training_log:{'do_train': True, 'do_valid': True, 'do_test': True, 'no_sparse': False, 'load_ckpt': False, 'featureless': False, 'use_sem': False, 'use_syn': False, 'use_seq': False, 'save_path': './saved_model', 'dataset': 'mr', 'model': 'gcn', 'lr': 2e-05, 'epochs': 300, 'hidden': 200, 'layers': 2, 'dropout': 0.8, 'weight_decay': 1e-06, 'early_stop': 100, 'max_degree': 3, 'model_name': 'model', 'run_id': '2024-04-20_01-33-29'}\n",
            "2024-04-20 10:55:15,747 - INFO - Optimization Finished!\n",
            "INFO:training_log:Optimization Finished!\n",
            "2024-04-20 10:55:15,762 - INFO - Successfully pickled file 'model_train_results.pkl' with loss and accuracy metrics to ./saved_model\n",
            "INFO:training_log:Successfully pickled file 'model_train_results.pkl' with loss and accuracy metrics to ./saved_model\n",
            "2024-04-20 10:55:15,767 - INFO - Starting validation\n",
            "INFO:training_log:Starting validation\n",
            "2024-04-20 10:55:16,168 - INFO - Val set results: cost= 0.61363 accuracy= 0.73099 time= 0.04176\n",
            "INFO:training_log:Val set results: cost= 0.61363 accuracy= 0.73099 time= 0.04176\n",
            "2024-04-20 10:55:16,176 - DEBUG - tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "DEBUG:training_log:tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "2024-04-20 10:55:16,239 - DEBUG - 29426\n",
            "DEBUG:training_log:29426\n",
            "2024-04-20 10:55:17,031 - INFO - Val Precision, Recall and F1-Score...\n",
            "INFO:training_log:Val Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:17,047 - INFO - \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8008    0.6006    0.6864       348\n",
            "           1     0.6904    0.8564    0.7645       362\n",
            "\n",
            "    accuracy                         0.7310       710\n",
            "   macro avg     0.7456    0.7285    0.7254       710\n",
            "weighted avg     0.7445    0.7310    0.7262       710\n",
            " \n",
            "INFO:training_log:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8008    0.6006    0.6864       348\n",
            "           1     0.6904    0.8564    0.7645       362\n",
            "\n",
            "    accuracy                         0.7310       710\n",
            "   macro avg     0.7456    0.7285    0.7254       710\n",
            "weighted avg     0.7445    0.7310    0.7262       710\n",
            " \n",
            "2024-04-20 10:55:17,051 - INFO - Macro average Val Precision, Recall and F1-Score...\n",
            "INFO:training_log:Macro average Val Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:17,059 - INFO - (0.7455947230542115, 0.7284641519019496, 0.7254296931153941, None)\n",
            "INFO:training_log:(0.7455947230542115, 0.7284641519019496, 0.7254296931153941, None)\n",
            "2024-04-20 10:55:17,068 - INFO - Micro average Val Precision, Recall and F1-Score...\n",
            "INFO:training_log:Micro average Val Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:17,076 - INFO - (0.7309859154929578, 0.7309859154929578, 0.7309859154929578, None)\n",
            "INFO:training_log:(0.7309859154929578, 0.7309859154929578, 0.7309859154929578, None)\n",
            "2024-04-20 10:55:17,081 - INFO - Starting testing\n",
            "INFO:training_log:Starting testing\n",
            "2024-04-20 10:55:17,510 - INFO - Test set results: cost= 0.60699 accuracy= 0.71666 time= 0.06247\n",
            "INFO:training_log:Test set results: cost= 0.60699 accuracy= 0.71666 time= 0.06247\n",
            "2024-04-20 10:55:17,516 - DEBUG - Test mask:\n",
            "DEBUG:training_log:Test mask:\n",
            "2024-04-20 10:55:17,520 - DEBUG - 29426\n",
            "DEBUG:training_log:29426\n",
            "2024-04-20 10:55:18,341 - INFO - Test Precision, Recall and F1-Score...\n",
            "INFO:training_log:Test Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:18,362 - INFO - \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7912    0.5886    0.6751      1777\n",
            "           1     0.6725    0.8447    0.7488      1777\n",
            "\n",
            "    accuracy                         0.7167      3554\n",
            "   macro avg     0.7319    0.7167    0.7119      3554\n",
            "weighted avg     0.7319    0.7167    0.7119      3554\n",
            "\n",
            "INFO:training_log:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7912    0.5886    0.6751      1777\n",
            "           1     0.6725    0.8447    0.7488      1777\n",
            "\n",
            "    accuracy                         0.7167      3554\n",
            "   macro avg     0.7319    0.7167    0.7119      3554\n",
            "weighted avg     0.7319    0.7167    0.7119      3554\n",
            "\n",
            "2024-04-20 10:55:18,366 - INFO - Macro average Test Precision, Recall and F1-Score...\n",
            "INFO:training_log:Macro average Test Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:18,379 - INFO - (0.731858227731416, 0.7166572875633089, 0.7119358178528771, None)\n",
            "INFO:training_log:(0.731858227731416, 0.7166572875633089, 0.7119358178528771, None)\n",
            "2024-04-20 10:55:18,383 - INFO - Micro average Test Precision, Recall and F1-Score...\n",
            "INFO:training_log:Micro average Test Precision, Recall and F1-Score...\n",
            "2024-04-20 10:55:18,396 - INFO - (0.7166572875633089, 0.7166572875633089, 0.7166572875633089, None)\n",
            "INFO:training_log:(0.7166572875633089, 0.7166572875633089, 0.7166572875633089, None)\n",
            "2024-04-20 10:55:18,401 - INFO - Total execution time: 119.62 seconds\n",
            "INFO:training_log:Total execution time: 119.62 seconds\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division, print_function\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn import metrics\n",
        "import deepdish as dd\n",
        "from tqdm import trange\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "def parse_args(args=None):\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Training and Testing Knowledge Graph Embedding Models',\n",
        "        usage='train.py [<args>] [-h | --help]'\n",
        "    )\n",
        "    parser.add_argument('--do_train', default = True)\n",
        "    parser.add_argument('--do_valid', default = True)\n",
        "    parser.add_argument('--do_test', default = True)\n",
        "    parser.add_argument('--no_sparse', action='store_true')\n",
        "    parser.add_argument(\"--load_ckpt\", action='store_true')\n",
        "    parser.add_argument('--featureless', action='store_true')\n",
        "    parser.add_argument('--use_sem', action='store_true')\n",
        "    parser.add_argument('--use_syn', action='store_true')\n",
        "    parser.add_argument('--use_seq', action='store_true')\n",
        "    parser.add_argument(\"--save_path\", type=str, default='./saved_model', help=\"the path of saved model\")\n",
        "    parser.add_argument('--dataset', type=str, default='mr', help='dataset name, default to mr')\n",
        "    parser.add_argument('--model', type=str, default='gcn', help='model name, default to gcn')\n",
        "    parser.add_argument('--lr', '--learning_rate', default=0.00002, type=float)   # 0.002/0.0002\n",
        "    parser.add_argument(\"--epochs\", default=300, type=int)\n",
        "    parser.add_argument(\"--hidden\", default=200, type=int)\n",
        "    parser.add_argument(\"--layers\", default=2, type=int)\n",
        "    parser.add_argument(\"--dropout\", default=0.8, type=float)   # 0.5/0.3/0.1\n",
        "    parser.add_argument(\"--weight_decay\", default=0.000001, type=float)\n",
        "    parser.add_argument(\"--early_stop\", default=300, type=int)\n",
        "    parser.add_argument(\"--max_degree\", default=3, type=int)\n",
        "    parser.add_argument(\"--model_name\", default='model', type=str)\n",
        "    parser.add_argument(\"--run_id\", default='2024-04-20_01-33-29', type=str)\n",
        "    return parser.parse_known_args(args)[0]\n",
        "\n",
        "def save_model(model, optimizer, args, timestamp):\n",
        "    '''\n",
        "    Save the parameters of the model   the optimizer,\n",
        "    as well as some other variables such as step and learning_rate\n",
        "    '''\n",
        "    if not os.path.exists(os.path.join(args.save_path,'run_{}'.format(timestamp))):\n",
        "        print(os.path.join(args.save_path,'run_{}'.format(timestamp)))\n",
        "        os.makedirs(os.path.join(args.save_path,'run_{}'.format(timestamp)))\n",
        "    argparse_dict = vars(args)\n",
        "    with open(os.path.join(args.save_path, 'run_{}/{}_config.json'.format(timestamp, args.model_name)), 'w') as fjson:\n",
        "        json.dump(argparse_dict, fjson)\n",
        "    logger.info(\"Configurations for training:\")\n",
        "    logger.debug(argparse_dict)\n",
        "\n",
        "    save_dict = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()}\n",
        "    torch.save(save_dict, os.path.join(args.save_path, 'run_{}/{}.bin'.format(timestamp, args.model_name)))\n",
        "\n",
        "    return True\n",
        "\n",
        "def train(args, features, train_label, train_mask, val_label, val_mask, test_label, test_mask, model, indice_list, weight_list)-> List[Dict]:\n",
        "    cost_train = []\n",
        "    cost_valid = []\n",
        "    cost_test = []\n",
        "    acc_train = []\n",
        "    acc_valid = []\n",
        "    acc_test = []\n",
        "\n",
        "    max_acc = 0.0\n",
        "    min_cost = 10.0\n",
        "    # for (name, param) in model.named_parameters():\n",
        "    #     print(name)\n",
        "    # weight_decay_list = (param for (name, param) in model.named_parameters() if 'layers.0' in name)\n",
        "    # no_decay_list = (param for (name, param) in model.named_parameters() if 'layers.0' not in name)\n",
        "    # parameters = [{'params':weight_decay_list},{'params':no_decay_list, 'weight_decay':0.0}]\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "    model.train()\n",
        "    for epoch in range(args.epochs):\n",
        "\n",
        "        t = time.time()\n",
        "        # Construct feed dictionary\n",
        "        # feed_dict = construct_feed_dict(\n",
        "        #     features, support, support_mix, y_train, train_mask, placeholders)\n",
        "        # feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "        # Training step\n",
        "        outs = model(features, indice_list, weight_list, 1-args.dropout)\n",
        "        pre_loss = loss_fct(outs, train_label)\n",
        "        train_pred = torch.argmax(outs, dim=-1)\n",
        "        ce_loss = (pre_loss * train_mask/train_mask.mean()).mean()\n",
        "        train_acc = ((train_pred == train_label).float() * train_mask/train_mask.mean()).mean()\n",
        "        # loss = ce_loss + tmp_loss\n",
        "        loss = ce_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
        "        model.eval()\n",
        "        # Validation\n",
        "        valid_cost, valid_acc, pred, labels, duration = evaluate(args,\n",
        "            features, val_label, val_mask, model, indice_list, weight_list)\n",
        "\n",
        "        # Testing\n",
        "        test_cost, test_acc, pred, labels, test_duration = evaluate(args,\n",
        "            features, test_label, test_mask, model, indice_list, weight_list)\n",
        "        model.train()\n",
        "\n",
        "        cost_valid.append(valid_cost)\n",
        "\n",
        "        cost_train.append(loss.item())\n",
        "        cost_test.append(test_cost)\n",
        "        acc_train.append(train_acc.item())\n",
        "        acc_valid.append(valid_acc)\n",
        "        acc_test.append(test_acc)\n",
        "\n",
        "        # logger.info(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()), \"train_acc=\",\n",
        "        #     \"{:.5f}\".format(train_acc.item()), \"val_loss=\", \"{:.5f}\".format(valid_cost),\n",
        "        #     \"val_acc=\", \"{:.5f}\".format(valid_acc), \"test_loss=\", \"{:.5f}\".format(test_cost), \"test_acc=\",\n",
        "        #     \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "        logger.info(\"\\n Epoch: {:04d} train_loss= {:.5f} train_acc= {:.5f} val_loss= {:.5f} val_acc= {:.5f} test_loss= {:.5f} test_acc= {:.5f} time= {:.5f}\".format(\n",
        "        epoch + 1, loss.item(), train_acc.item(), valid_cost, valid_acc, test_cost, test_acc, time.time() - t))\n",
        "\n",
        "\n",
        "        # save model\n",
        "        # if epoch > 700 and cost_valid[-1] < min_cost:\n",
        "        if cost_valid[-1] < min_cost:\n",
        "            saved_res = save_model(model, optimizer, args, timestamp)\n",
        "            min_cost = cost_valid[-1]\n",
        "            logger.info(\"Current best loss {:.5f}\".format(min_cost))\n",
        "\n",
        "        else:\n",
        "            saved_res = False\n",
        "\n",
        "        # if acc_valid[-1] > max_acc:\n",
        "        #     save_model(model, optimizer, args)\n",
        "        #     min_cost = cost_valid[-1]\n",
        "        #     max_acc = acc_valid[-1]\n",
        "        #     print(\"Current best acc {:.5f}\".format(max_acc))\n",
        "\n",
        "        # early stoppage implementation\n",
        "        # training loop terminates if validation cost exceeds mean of previous epochs\n",
        "        if epoch > args.early_stop and cost_valid[-1] > np.mean(cost_valid[-(args.early_stop + 1):-1]):\n",
        "            logger.info(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "    if not saved_res:\n",
        "        save_model(model, optimizer, args, timestamp)\n",
        "    logger.info(\"Optimization Finished!\")\n",
        "\n",
        "    loss_results = {\n",
        "        'train_loss': cost_train,\n",
        "        'valid_loss': cost_valid,\n",
        "        'test_loss': cost_test\n",
        "    }\n",
        "\n",
        "    acc_results = {\n",
        "        'train_acc': acc_train,\n",
        "        'valid_acc': acc_valid,\n",
        "        'test_acc': acc_test\n",
        "    }\n",
        "\n",
        "    return [loss_results, acc_results]\n",
        "\n",
        "def evaluate(args, features, label, mask, model, indice_list, weight_list):\n",
        "    t_test = time.time()\n",
        "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "    with torch.no_grad():\n",
        "        outs = model(features, indice_list, weight_list, 1)\n",
        "        pre_loss = loss_fct(outs, label)\n",
        "        pred = torch.argmax(outs, dim=-1)\n",
        "        ce_loss = (pre_loss * mask/mask.mean()).mean()\n",
        "        loss = ce_loss\n",
        "        acc = ((pred == label).float() * mask/mask.mean()).mean()\n",
        "    # feed_dict_val = construct_feed_dict(\n",
        "    #     features, support, support_mix, labels, mask, placeholders)\n",
        "    # outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
        "    return loss.item(), acc.item(), pred.cpu().numpy(), label.cpu().numpy(), (time.time() - t_test)\n",
        "\n",
        "def load_ckpt(model):\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = dd.io.load('./gcn.h5')\n",
        "    model_dict['layers.0.intra_convs.0'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_0:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.0.inter_convs.0'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_00:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.0.intra_convs.1'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_1:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.0.inter_convs.1'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_11:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.0.intra_convs.2'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_2:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.0.inter_convs.2'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_1_vars_weights_22:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.intra_convs.0'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_0:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.inter_convs.0'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_00:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.intra_convs.1'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_1:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.inter_convs.1'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_11:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.intra_convs.2'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_2:0'].T, dtype=torch.float)\n",
        "    model_dict['layers.1.inter_convs.2'] = torch.tensor(pretrained_dict['gcn_graphconvolution_mix1_2_vars_weights_22:0'].T, dtype=torch.float)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "def get_edge_tensor(adj):\n",
        "    row = torch.tensor(adj.row, dtype=torch.long)\n",
        "    col = torch.tensor(adj.col, dtype=torch.long)\n",
        "    data = torch.tensor(adj.data, dtype=torch.float)\n",
        "    indice = torch.stack((row,col),dim=0)\n",
        "    return indice, data\n",
        "\n",
        "\n",
        "def main(args, timestamp):\n",
        "    start_time = time.time()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "        logger.info(\"Training is running on {}\".format(device))\n",
        "    else:\n",
        "        device = None\n",
        "        logger.info(\"Training is running on CPU\")\n",
        "\n",
        "    # Set random seed\n",
        "    seed=147\n",
        "    logger.info(\"Seed used: {}\".format(seed))\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed(seed)\n",
        "    # Load data\n",
        "    # adj, adj1, adj2, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, val_size,test_size, num_labels = load_corpus_torch(args, device)\n",
        "    adj_lst, y_train, y_val, y_test, train_mask, val_mask, test_mask, train_size, val_size,test_size, num_labels = load_corpus_torch(args, device)\n",
        "    # for adj in adj_lst:\n",
        "    #     adj = adj.tocoo()\n",
        "    # adj = adj.tocoo()\n",
        "    # adj1 = adj1.tocoo()\n",
        "    # adj2 = adj2.tocoo()\n",
        "    logger.debug(\"adj:\\n {}\".format(adj_lst[0]))\n",
        "\n",
        "    logger.info(\"The shape of adj is {}\".format(adj_lst[0].shape))\n",
        "\n",
        "    # one-hot features\n",
        "    # features = torch.eye(adj.shape[0], dtype=torch.float).to_sparse().to(device)\n",
        "    # support_mix = [adj, adj1, adj2]\n",
        "    # support_mix = adj_lst\n",
        "    indice_list, weight_list = [] , []\n",
        "    for adjacency in adj_lst:\n",
        "        adjacency = adjacency.tocoo()\n",
        "        ind, dat = get_edge_tensor(adjacency)\n",
        "        indice_list.append(ind.to(device))\n",
        "        weight_list.append(dat.to(device))\n",
        "\n",
        "    in_dim = adj_lst[0].shape[0]\n",
        "    model = TGCN(in_dim=in_dim, hidden_dim=args.hidden, out_dim=num_labels, num_graphs=len(adj_lst), dropout=args.dropout, n_layers=args.layers, bias=False, featureless=args.featureless)\n",
        "    features = torch.tensor(list(range(in_dim)), dtype=torch.long).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    if args.do_train:\n",
        "        logger.info(\"Starting training\")\n",
        "        results = train(args, features, y_train, train_mask, y_val, val_mask, y_test, test_mask, model, indice_list, weight_list)\n",
        "\n",
        "        with open(os.path.join(args.save_path ,'run_{}/{}_train_results.pkl'.format(timestamp, args.model_name)), 'wb') as f:\n",
        "            pkl.dump(results, f)\n",
        "        logger.info(\"Successfully pickled file '{}_train_results.pkl' with loss and accuracy metrics to {}\".format(args.model_name, args.save_path))\n",
        "\n",
        "    if args.do_valid:\n",
        "        logger.info(\"Starting validation\")\n",
        "        # FLAGS.dropout = 1.0\n",
        "        save_dict = torch.load(os.path.join(args.save_path, 'run_{}/{}.bin'.format(timestamp, args.model_name)))\n",
        "        if args.load_ckpt:\n",
        "            load_ckpt(model)\n",
        "        else:\n",
        "            model.load_state_dict(save_dict['model_state_dict'])\n",
        "        model.eval()\n",
        "        # Testing\n",
        "        val_cost, val_acc, pred, labels, val_duration = evaluate(args,\n",
        "            features, y_val, val_mask, model, indice_list, weight_list)\n",
        "        # logger.info(\"Val set results:\", \"cost=\", \"{:.5f}\".format(val_cost),\n",
        "        #     \"accuracy=\", \"{:.5f}\".format(val_acc), \"time=\", \"{:.5f}\".format(val_duration))\n",
        "        logger.info(\"Val set results: cost= {:.5f} accuracy= {:.5f} time= {:.5f}\".format(val_cost, val_acc, val_duration))\n",
        "\n",
        "        val_pred = []\n",
        "        val_labels = []\n",
        "        logger.debug(val_mask)\n",
        "        logger.debug(len(val_mask))\n",
        "        for i in range(len(val_mask)):\n",
        "            if val_mask[i] == 1:\n",
        "                val_pred.append(pred[i])\n",
        "                val_labels.append(labels[i])\n",
        "\n",
        "        logger.info(\"Val Precision, Recall and F1-Score...\")\n",
        "        logger.info(\"\\n {} \".format(metrics.classification_report(val_labels, val_pred, digits=4)))\n",
        "        logger.info(\"Macro average Val Precision, Recall and F1-Score...\")\n",
        "        logger.info(metrics.precision_recall_fscore_support(val_labels, val_pred, average='macro'))\n",
        "        logger.info(\"Micro average Val Precision, Recall and F1-Score...\")\n",
        "        logger.info(metrics.precision_recall_fscore_support(val_labels, val_pred, average='micro'))\n",
        "\n",
        "    if args.do_test:\n",
        "        # FLAGS.dropout = 1.0\n",
        "        logger.info(\"Starting testing\")\n",
        "        save_dict = torch.load(os.path.join(args.save_path, 'run_{}/{}.bin'.format(timestamp, args.model_name)))\n",
        "        if args.load_ckpt:\n",
        "            load_ckpt(model)\n",
        "        else:\n",
        "            model.load_state_dict(save_dict['model_state_dict'])\n",
        "        model.eval()\n",
        "        # Testing\n",
        "        test_cost, test_acc, pred, labels, test_duration = evaluate(args,\n",
        "            features, y_test, test_mask, model, indice_list, weight_list)\n",
        "        # logger.info(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "        #     \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
        "        logger.info(\"Test set results: cost= {:.5f} accuracy= {:.5f} time= {:.5f}\".format(test_cost, test_acc, test_duration))\n",
        "\n",
        "        test_pred = []\n",
        "        test_labels = []\n",
        "        logger.debug(\"Test mask:\")\n",
        "        logger.debug(len(test_mask))\n",
        "        for i in range(len(test_mask)):\n",
        "            if test_mask[i] == 1:\n",
        "                test_pred.append(pred[i])\n",
        "                test_labels.append(labels[i])\n",
        "\n",
        "        logger.info(\"Test Precision, Recall and F1-Score...\")\n",
        "        logger.info('\\n {}'.format(metrics.classification_report(test_labels, test_pred, digits=4)))\n",
        "        logger.info(\"Macro average Test Precision, Recall and F1-Score...\")\n",
        "        logger.info(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
        "        logger.info(\"Micro average Test Precision, Recall and F1-Score...\")\n",
        "        logger.info(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
        "    end_time = time.time()\n",
        "    logger.info(\"Total execution time: {} seconds\".format(round(end_time-start_time,2)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # retrieve execution timestamp for logs\n",
        "    sgt = pytz.timezone('Asia/Singapore')\n",
        "    timestamp = datetime.now(sgt).strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    # set up logging\n",
        "    log_path = os.path.join(Path(os.path.abspath(os.path.dirname(\"__file__\")), '../logs'))\n",
        "    logger = setup_logging(log_path=log_path, log_name='training_log', timestamp=timestamp)\n",
        "\n",
        "    main(parse_args(), timestamp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJEF0q8PZt5A",
        "outputId": "c7c2d50f-8886-499e-9cf7-09e8a334a205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haTaLdjzQNS7",
        "outputId": "f3ea47d8-3301-4546-c68d-d940c432450d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive\n"
          ]
        }
      ],
      "source": [
        "%cd ./gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQm9GS_cZwvm",
        "outputId": "2fce60d2-e74d-4de6-940a-fc4e0de7d9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/dl\n"
          ]
        }
      ],
      "source": [
        "%cd ./MyDrive/dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1fJiCCpZ1CU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
